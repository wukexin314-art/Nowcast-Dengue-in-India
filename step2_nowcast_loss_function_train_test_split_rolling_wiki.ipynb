{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f629dbf",
   "metadata": {},
   "source": [
    "# Step 2 Dengue Nowcasting (ARX + Joint Loss)\n",
    "\n",
    "This notebook implements Step 2 according to the requirements:\n",
    "- Training set: months **<= split_month** (rolling splits)\n",
    "- Test set: months **> split_month** (for evaluation only)\n",
    "- Joint loss: WHO monthly supervision + OpenDengue yearly constraints + L2 regularization\n",
    "- Use Step 1 estimates to impute missing WHO observations as $\\tilde{x}$ for lagged inputs\n",
    "- Output complete parameters, training curves, predictions, and plots\n",
    "\n",
    "Overfitting control: L2 regularization + early stopping + train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae4f6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install dependencies in your environment:\n",
    "# pip install -U pandas numpy torch matplotlib\n",
    "\n",
    "# Prevent kernel crashes from OpenMP/MKL library conflicts\n",
    "import os\n",
    "os.environ.setdefault('KMP_DUPLICATE_LIB_OK', 'TRUE')\n",
    "os.environ.setdefault('OMP_NUM_THREADS', '1')\n",
    "os.environ.setdefault('MKL_NUM_THREADS', '1')\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Further limit thread count\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b1261ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Step2Config(data_path='master_data.csv', step1_predictions_path='outputs_step1_wiki/predictions_step1_monthly.csv', step1_pred_col='x_pred', start_month='2021-01', rolling_start='2024-11', rolling_end='2025-08', rolling_max_epochs=8000, rolling_patience=400, target_source='WHO', google_sources=('Google_Trends_Dengue_fever', 'Google_Trends_Dengue_vaccine'), use_wiki=True, wiki_path='total_dengue_views.csv', wiki_month_col='Month', wiki_value_col='Total_Views', wiki_transform='log1p', lags_y=(1, 2), use_month_dummies=True, yearly_proxy_sources_priority=('OpenDengue_State_Aggregated', 'OpenDengue_National_Yearly'), lambda_who=5.0, lambda_year=5.0, lambda_reg=0.0001, lambda_lag_reg=0.005, epochs=30000, lr=0.003, seed=42, target_scale=1000.0, early_stop=True, patience=1000, min_delta=1e-07, standardize_features=True, outdir='outputs_step2_jointloss_wiki', clip_nonnegative=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Step2Config:\n",
    "    # Inputs\n",
    "    data_path: str = 'master_data.csv'\n",
    "    step1_predictions_path: str = os.path.join('outputs_step1_wiki', 'predictions_step1_monthly.csv')\n",
    "    step1_pred_col: str = 'x_pred'\n",
    "\n",
    "    # Modeling window\n",
    "    start_month: str = '2021-01'\n",
    "\n",
    "    # Rolling split evaluation (inclusive)\n",
    "    rolling_start: str = '2024-11'\n",
    "    rolling_end: str = '2025-08'\n",
    "    # To keep rolling evaluation runtime reasonable, you can override training budget per split\n",
    "    rolling_max_epochs: int = 8000\n",
    "    rolling_patience: int = 400\n",
    "\n",
    "    # Target/source names in master_data.csv\n",
    "    target_source: str = 'WHO'\n",
    "\n",
    "    # Exogenous signals\n",
    "    google_sources: Tuple[str, str] = (\n",
    "        'Google_Trends_Dengue_fever',\n",
    "        'Google_Trends_Dengue_vaccine',\n",
    "    )\n",
    "\n",
    "\n",
    "    # Wikipedia pageviews (external regressor)\n",
    "    use_wiki: bool = True\n",
    "    wiki_path: str = 'total_dengue_views.csv'\n",
    "    wiki_month_col: str = 'Month'\n",
    "    wiki_value_col: str = 'Total_Views'\n",
    "    wiki_transform: str = 'log1p'  # 'log1p' or 'none'\n",
    "\n",
    "    # Feature engineering\n",
    "    lags_y: Tuple[int, ...] = (1, 2)  # Back to both lags\n",
    "    use_month_dummies: bool = True\n",
    "\n",
    "    # OpenDengue yearly proxy sources (priority)\n",
    "    yearly_proxy_sources_priority: Tuple[str, ...] = (\n",
    "        'OpenDengue_State_Aggregated',\n",
    "        'OpenDengue_National_Yearly',\n",
    "    )\n",
    "\n",
    "    # Loss weights - Conservative: balance all three components\n",
    "    lambda_who: float = 5.0  # Moderate WHO weight\n",
    "    lambda_year: float = 5.0  # Strong yearly constraint to anchor predictions\n",
    "    lambda_reg: float = 1e-4  # Base regularization\n",
    "    lambda_lag_reg: float = 5e-3  # Moderate lag regularization\n",
    "\n",
    "    # Optimization\n",
    "    epochs: int = 30000  # Sufficient epochs for convergence\n",
    "    lr: float = 3e-3  # Slower learning rate for stability\n",
    "    seed: int = 42\n",
    "    target_scale: float = 1000.0\n",
    "\n",
    "    # Overfitting controls\n",
    "    early_stop: bool = True\n",
    "    patience: int = 1000  # Large patience for stable convergence\n",
    "    min_delta: float = 1e-7  # Smaller threshold for early stopping\n",
    "\n",
    "    # Scaling features (standardize using train stats)\n",
    "    standardize_features: bool = True\n",
    "\n",
    "    # Output\n",
    "    outdir: str = 'outputs_step2_jointloss_wiki'\n",
    "    clip_nonnegative: bool = True\n",
    "\n",
    "\n",
    "cfg = Step2Config()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3e6b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_outdir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def parse_monthly_date_any(s: pd.Series) -> pd.Series:\n",
    "    x = s.astype(str)\n",
    "    is_ym = x.str.match('^[0-9]{4}-[0-9]{2}$')\n",
    "    x = np.where(is_ym, x + '-01', x)\n",
    "    dt = pd.to_datetime(x, errors='coerce')\n",
    "    return dt\n",
    "\n",
    "\n",
    "def load_master_csv(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    expected = {'resolution', 'date', 'value', 'source'}\n",
    "    missing = expected - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"master_data.csv missing columns: {sorted(missing)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_monthly_wide(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    m = df[df['resolution'].astype(str).str.lower().eq('monthly')].copy()\n",
    "    m['date'] = parse_monthly_date_any(m['date'])\n",
    "    m = m.dropna(subset=['date']).copy()\n",
    "    wide = (\n",
    "        m.pivot_table(index='date', columns='source', values='value', aggfunc='mean')\n",
    "        .sort_index()\n",
    "    )\n",
    "    return wide\n",
    "\n",
    "\n",
    "def build_yearly_proxy(df: pd.DataFrame, priority_sources: Tuple[str, ...]) -> pd.DataFrame:\n",
    "    y = df[df['resolution'].astype(str).str.lower().eq('yearly')].copy()\n",
    "    y['year'] = pd.to_numeric(y['date'], errors='coerce').astype('Int64')\n",
    "    y = y.dropna(subset=['year'])\n",
    "    y['year'] = y['year'].astype(int)\n",
    "\n",
    "    pivot = (\n",
    "        y.pivot_table(index='year', columns='source', values='value', aggfunc='mean')\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    chosen = []\n",
    "    for year, row in pivot.iterrows():\n",
    "        val = np.nan\n",
    "        src = None\n",
    "        for s in priority_sources:\n",
    "            if s in row.index and pd.notna(row[s]):\n",
    "                val = float(row[s])\n",
    "                src = s\n",
    "                break\n",
    "        if pd.notna(val):\n",
    "            chosen.append((year, val, src))\n",
    "\n",
    "    return pd.DataFrame(chosen, columns=['year', 'od_total', 'od_source'])\n",
    "\n",
    "\n",
    "def load_step1_predictions(cfg: Step2Config) -> pd.Series:\n",
    "    if not os.path.exists(cfg.step1_predictions_path):\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    s1 = pd.read_csv(cfg.step1_predictions_path)\n",
    "    if 'date' not in s1.columns:\n",
    "        raise ValueError(f\"Step 1 predictions file missing 'date': {cfg.step1_predictions_path}\")\n",
    "\n",
    "    s1['date'] = parse_monthly_date_any(s1['date'])\n",
    "    s1 = s1.dropna(subset=['date']).copy()\n",
    "    s1 = s1.sort_values('date')\n",
    "\n",
    "    col = cfg.step1_pred_col if cfg.step1_pred_col in s1.columns else None\n",
    "    if col is None:\n",
    "        cand = [c for c in s1.columns if 'pred' in c.lower()]\n",
    "        if not cand:\n",
    "            cand = [c for c in s1.columns if c != 'date']\n",
    "        if not cand:\n",
    "            raise ValueError('No prediction column found in Step 1 predictions file.')\n",
    "        col = cand[0]\n",
    "\n",
    "    ser = s1.set_index('date')[col].astype(float)\n",
    "    return ser\n",
    "\n",
    "\n",
    "\n",
    "def load_wiki_monthly(cfg: Step2Config) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Load monthly Wikipedia pageviews time series.\n",
    "    Expected columns: cfg.wiki_month_col (YYYY-MM) and cfg.wiki_value_col (numeric)\n",
    "    Returns: pd.Series indexed by month (Timestamp at month start).\n",
    "    \"\"\"\n",
    "    if (not getattr(cfg, 'use_wiki', False)) or (not os.path.exists(cfg.wiki_path)):\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    w = pd.read_csv(cfg.wiki_path)\n",
    "    if cfg.wiki_month_col not in w.columns or cfg.wiki_value_col not in w.columns:\n",
    "        raise ValueError(\n",
    "            f\"Wiki file must contain columns '{cfg.wiki_month_col}' and '{cfg.wiki_value_col}'. Got: {list(w.columns)}\"\n",
    "        )\n",
    "\n",
    "    w = w.copy()\n",
    "    w['date'] = parse_monthly_date_any(w[cfg.wiki_month_col])\n",
    "    w = w.dropna(subset=['date']).sort_values('date')\n",
    "    s = pd.to_numeric(w[cfg.wiki_value_col], errors='coerce')\n",
    "    ser = pd.Series(s.values, index=w['date']).groupby(level=0).mean()\n",
    "    return ser\n",
    "\n",
    "def add_month_dummies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dt = pd.to_datetime(df.index)\n",
    "    m = pd.get_dummies(dt.month, prefix='m', drop_first=True)\n",
    "    m.index = df.index\n",
    "    return pd.concat([df, m.astype(float)], axis=1)\n",
    "\n",
    "\n",
    "def build_design_matrix(wide: pd.DataFrame, cfg: Step2Config) -> tuple[pd.DataFrame, np.ndarray, np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Build design matrix.\n",
    "    Critical fix: lagged variables x_tilde must be scaled by target_scale to match target scale.\n",
    "    \"\"\"\n",
    "    start_dt = pd.to_datetime(cfg.start_month + '-01')\n",
    "    wide = wide.loc[wide.index >= start_dt].copy()\n",
    "\n",
    "    target = cfg.target_source\n",
    "    if target not in wide.columns:\n",
    "        wide[target] = np.nan\n",
    "\n",
    "    g1, g2 = cfg.google_sources\n",
    "    for g in [g1, g2]:\n",
    "        if g not in wide.columns:\n",
    "            raise ValueError(f\"Missing Google source '{g}' in monthly data.\")\n",
    "\n",
    "    y_who = pd.to_numeric(wide[target], errors='coerce')\n",
    "\n",
    "    step1_ser = load_step1_predictions(cfg)\n",
    "    wide['STEP1_est'] = step1_ser.reindex(wide.index)\n",
    "\n",
    "    # x_tilde: WHO when observed, else Step1 estimate (for lagged inputs)\n",
    "    x_tilde = y_who.where(y_who.notna(), wide['STEP1_est'])\n",
    "    \n",
    "    # Critical fix: scale lagged variables to match target scale\n",
    "    x_tilde_scaled = x_tilde / cfg.target_scale\n",
    "\n",
    "    X_df = pd.DataFrame(index=wide.index)\n",
    "    \n",
    "    # Normalize Google Trends from [0, 100] to [0, 1]\n",
    "    X_df['g_fever'] = pd.to_numeric(wide[g1], errors='coerce') / 100.0\n",
    "    X_df['g_vaccine'] = pd.to_numeric(wide[g2], errors='coerce') / 100.0\n",
    "\n",
    "\n",
    "    # Wikipedia pageviews regressor\n",
    "    if getattr(cfg, 'use_wiki', False):\n",
    "        wiki_ser = load_wiki_monthly(cfg)\n",
    "        wide['WIKI_raw'] = wiki_ser.reindex(wide.index)\n",
    "        wiki_raw = pd.to_numeric(wide['WIKI_raw'], errors='coerce')\n",
    "        wiki_raw = wiki_raw.interpolate(limit_direction='both').ffill().bfill()\n",
    "        if str(getattr(cfg, 'wiki_transform', 'log1p')).lower() == 'log1p':\n",
    "            X_df['wiki_views'] = np.log1p(wiki_raw.astype(float))\n",
    "        else:\n",
    "            X_df['wiki_views'] = wiki_raw.astype(float)\n",
    "\n",
    "    # Use scaled values for lagged variables\n",
    "    for k in cfg.lags_y:\n",
    "        X_df[f'x_tilde_lag{k}'] = x_tilde_scaled.shift(k)\n",
    "\n",
    "    if cfg.use_month_dummies:\n",
    "        X_df = add_month_dummies(X_df)\n",
    "\n",
    "    X_df['intercept'] = 1.0\n",
    "\n",
    "    # Light imputation on exogenous features only (lags can remain NaN and be imputed later)\n",
    "    exo_cols = ['g_fever', 'g_vaccine'] + (['wiki_views'] if 'wiki_views' in X_df.columns else [])\n",
    "    for col in exo_cols:\n",
    "        X_df[col] = X_df[col].interpolate(limit_direction='both').ffill().bfill()\n",
    "\n",
    "    feature_cols = [c for c in X_df.columns if c != 'intercept'] + ['intercept']\n",
    "    X = X_df[feature_cols].to_numpy(dtype=np.float32)\n",
    "    y = y_who.to_numpy(dtype=np.float32)\n",
    "    mask_who = np.isfinite(y)\n",
    "\n",
    "    return X_df, X, y, mask_who, feature_cols\n",
    "\n",
    "\n",
    "def build_year_constraints(dates: pd.DatetimeIndex, yearly_proxy: pd.DataFrame, train_mask: np.ndarray) -> List[tuple]:\n",
    "    df_dates = pd.DataFrame({'date': pd.to_datetime(dates)})\n",
    "    df_dates['year'] = df_dates['date'].dt.year\n",
    "    df_dates['month'] = df_dates['date'].dt.month\n",
    "\n",
    "    constraints = []\n",
    "    for _, r in yearly_proxy.iterrows():\n",
    "        y = int(r['year'])\n",
    "        od_total = float(r['od_total'])\n",
    "        od_src = str(r['od_source'])\n",
    "\n",
    "        idx = df_dates.index[df_dates['year'].eq(y)].to_numpy()\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "\n",
    "        # require all 12 months and all months in training window\n",
    "        months_present = set(df_dates.loc[idx, 'month'].tolist())\n",
    "        if months_present != set(range(1, 13)):\n",
    "            continue\n",
    "        if not np.all(train_mask[idx]):\n",
    "            continue\n",
    "\n",
    "        constraints.append((y, idx, od_total, od_src))\n",
    "\n",
    "    return constraints\n",
    "\n",
    "\n",
    "def time_series_split_mask(index: pd.DatetimeIndex, split_month: str):\n",
    "    split_dt = pd.to_datetime(split_month + '-01')\n",
    "    train_mask = index <= split_dt\n",
    "    test_mask = index > split_dt\n",
    "    return train_mask, test_mask\n",
    "\n",
    "\n",
    "def standardize_features(X: np.ndarray, train_mask: np.ndarray, skip_cols: List[int] = None):\n",
    "    \"\"\"\n",
    "    Standardize feature matrix.\n",
    "    skip_cols: column indices to skip standardization (e.g., intercept term).\n",
    "    \"\"\"\n",
    "    X2 = X.copy()\n",
    "    n, p = X2.shape\n",
    "    means = np.zeros(p, dtype=np.float32)\n",
    "    stds = np.ones(p, dtype=np.float32)\n",
    "    \n",
    "    if skip_cols is None:\n",
    "        skip_cols = []\n",
    "\n",
    "    for j in range(p):\n",
    "        col = X2[:, j]\n",
    "        col_train = col[train_mask]\n",
    "        \n",
    "        # For intercept or specified columns, skip standardization\n",
    "        if j in skip_cols:\n",
    "            # Only impute NaN\n",
    "            col = np.where(np.isfinite(col), col, 0.0)\n",
    "            X2[:, j] = col\n",
    "            continue\n",
    "        \n",
    "        m = np.nanmean(col_train)\n",
    "        s = np.nanstd(col_train)\n",
    "        if not np.isfinite(m):\n",
    "            m = 0.0\n",
    "        if (not np.isfinite(s)) or s <= 1e-12:\n",
    "            s = 1.0\n",
    "        means[j] = m\n",
    "        stds[j] = s\n",
    "\n",
    "        # impute NaNs with train mean, then standardize\n",
    "        col = np.where(np.isfinite(col), col, m)\n",
    "        X2[:, j] = (col - m) / s\n",
    "\n",
    "    return X2, means, stds\n",
    "\n",
    "\n",
    "def train_step2_joint_loss(\n",
    "    X: np.ndarray,\n",
    "    y_who: np.ndarray,\n",
    "    mask_who: np.ndarray,\n",
    "    year_constraints: List[tuple],\n",
    "    train_mask: np.ndarray,\n",
    "    cfg: Step2Config,\n",
    "    feature_cols: List[str],\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train with differentiated regularization: stronger penalty on lag coefficients.\n",
    "    \"\"\"\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    y_scaled = y_who / cfg.target_scale\n",
    "    y_t = torch.tensor(y_scaled, dtype=torch.float32, device=device)\n",
    "    mask_who_t = torch.tensor(mask_who & train_mask, dtype=torch.bool, device=device)\n",
    "\n",
    "    year_terms = []\n",
    "    for (year, idx, od_total, od_src) in year_constraints:\n",
    "        year_terms.append(\n",
    "            (year, torch.tensor(idx, dtype=torch.long, device=device), float(od_total / cfg.target_scale), od_src)\n",
    "        )\n",
    "\n",
    "    # Identify lag feature indices for differentiated regularization\n",
    "    lag_indices = [i for i, col in enumerate(feature_cols) if 'lag' in col.lower()]\n",
    "\n",
    "    p = X_t.shape[1]\n",
    "    beta = torch.nn.Parameter(torch.zeros(p, dtype=torch.float32, device=device))\n",
    "    opt = torch.optim.Adam([beta], lr=cfg.lr)\n",
    "\n",
    "    rows = []\n",
    "    best_loss = np.inf\n",
    "    best_beta = None\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        x = X_t @ beta\n",
    "\n",
    "        if mask_who_t.any():\n",
    "            diff_who = x[mask_who_t] - y_t[mask_who_t]\n",
    "            L_who = (diff_who ** 2).mean()\n",
    "        else:\n",
    "            L_who = torch.tensor(0.0, device=device)\n",
    "\n",
    "        if len(year_terms) > 0:\n",
    "            diffs = []\n",
    "            for _, idx_t, od_total_scaled, _ in year_terms:\n",
    "                year_sum = x.index_select(0, idx_t).sum()\n",
    "                diffs.append((year_sum - od_total_scaled) ** 2)\n",
    "            L_year = torch.stack(diffs).mean()\n",
    "        else:\n",
    "            L_year = torch.tensor(0.0, device=device)\n",
    "\n",
    "        # Differentiated regularization: base + extra penalty on lag coefficients\n",
    "        L_reg_base = (beta ** 2).sum()\n",
    "        if lag_indices:\n",
    "            beta_lag = beta[lag_indices]\n",
    "            L_reg_lag = (beta_lag ** 2).sum()\n",
    "        else:\n",
    "            L_reg_lag = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        L_reg = L_reg_base\n",
    "        L_total = cfg.lambda_who * L_who + cfg.lambda_year * L_year + cfg.lambda_reg * L_reg + cfg.lambda_lag_reg * L_reg_lag\n",
    "        L_total.backward()\n",
    "        opt.step()\n",
    "\n",
    "        val = float(L_total.detach().cpu().item())\n",
    "        rows.append({\n",
    "            'epoch': epoch,\n",
    "            'L_total': val,\n",
    "            'L_who': float(L_who.detach().cpu().item()),\n",
    "            'L_year': float(L_year.detach().cpu().item()),\n",
    "            'L_reg': float(L_reg.detach().cpu().item()),\n",
    "            'L_lag_reg': float(L_reg_lag.detach().cpu().item()) if lag_indices else 0.0,\n",
    "        })\n",
    "\n",
    "        if not np.isfinite(val):\n",
    "            raise RuntimeError('Training diverged (loss is NaN/Inf).')\n",
    "\n",
    "        if cfg.early_stop:\n",
    "            if val < best_loss - cfg.min_delta:\n",
    "                best_loss = val\n",
    "                best_beta = beta.detach().cpu().numpy().copy()\n",
    "                bad_epochs = 0\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "                if bad_epochs >= cfg.patience:\n",
    "                    break\n",
    "\n",
    "    beta_hat = beta.detach().cpu().numpy() if best_beta is None else best_beta\n",
    "    loss_df = pd.DataFrame(rows)\n",
    "    return beta_hat, loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d569c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.sqrt(np.nanmean((y_true - y_pred) ** 2)))\n",
    "\n",
    "\n",
    "def safe_mape(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = np.where(np.abs(y_true) < 1e-12, np.nan, np.abs(y_true))\n",
    "    return float(np.nanmean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "\n",
    "\n",
    "def make_predictions(X: np.ndarray, beta_hat: np.ndarray, cfg: Step2Config) -> np.ndarray:\n",
    "    x_scaled = X @ beta_hat\n",
    "    x = x_scaled * cfg.target_scale\n",
    "    if cfg.clip_nonnegative:\n",
    "        x = np.maximum(x, 0.0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def plot_loss_curve(loss_df: pd.DataFrame, outdir: Path) -> None:\n",
    "    fig = plt.figure()\n",
    "    plt.plot(loss_df['epoch'], loss_df['L_total'], label='Total')\n",
    "    if (loss_df['L_who'] != 0).any():\n",
    "        plt.plot(loss_df['epoch'], loss_df['L_who'], label='WHO')\n",
    "    if (loss_df['L_year'] != 0).any():\n",
    "        plt.plot(loss_df['epoch'], loss_df['L_year'], label='Yearly')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (log scale)')\n",
    "    plt.title('Training Loss Curve (Step 2)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outdir / 'loss_curve_step2.png', dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_who_vs_pred(dates: pd.DatetimeIndex, y_who: np.ndarray, x_pred: np.ndarray, train_mask: np.ndarray, outdir: Path, fname: str = 'who_vs_pred_step2.png', title: str = None) -> None:\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    plt.plot(dates, x_pred, label='Predicted (Step 2)')\n",
    "    plt.plot(dates, y_who, label='WHO observed')\n",
    "    split_dt = dates[train_mask].max() if train_mask.any() else None\n",
    "    if split_dt is not None:\n",
    "        plt.axvline(split_dt, linestyle='--', color='gray', label='Train/Test split')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Monthly dengue cases')\n",
    "    plt.title(title if title is not None else 'WHO vs Prediction (Step 2)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outdir / fname, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_yearly_vs_od(pred_df: pd.DataFrame, yearly_proxy: pd.DataFrame, outdir: Path) -> None:\n",
    "    pred_year = pred_df.groupby('year', as_index=False)['x_pred'].sum().rename(columns={'x_pred': 'pred_year_total'})\n",
    "    if yearly_proxy.empty:\n",
    "        return\n",
    "    merged = pred_year.merge(yearly_proxy, on='year', how='inner').sort_values('year')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(merged['year'], merged['pred_year_total'], marker='o', label='Predicted yearly sum')\n",
    "    plt.plot(merged['year'], merged['od_total'], marker='o', label='OpenDengue yearly total')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Total dengue cases (year)')\n",
    "    plt.title('Yearly Aggregation: Prediction vs OpenDengue (Step 2)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outdir / 'yearly_vs_opendengue_step2.png', dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    merged.to_csv(outdir / 'yearly_comparison_step2.csv', index=False)\n",
    "\n",
    "\n",
    "def plot_step1_vs_step2(pred_df: pd.DataFrame, outdir: Path):\n",
    "    if 'x_step1' not in pred_df.columns:\n",
    "        return\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    dates = pd.to_datetime(pred_df['date'] + '-01')\n",
    "    plt.plot(dates, pred_df['x_step1'], label='Step 1')\n",
    "    plt.plot(dates, pred_df['x_pred'], label='Step 2')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Monthly dengue cases')\n",
    "    plt.title('Step 1 vs Step 2 Predictions')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outdir / 'step1_vs_step2.png', dpi=200)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84fae24",
   "metadata": {},
   "source": [
    "## 1) Load Data, Build Features, and Split Train/Test\n",
    "\n",
    "- Use Step 1 predictions to impute missing WHO observations for $\\tilde{x}$\n",
    "- Build ARX features: Google Trends + lagged variables + month dummies\n",
    "- Training/Test split is evaluated over a rolling set of split months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d799a3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total months: 60\n",
      "WHO observed months: 23\n",
      "Feature count: 17\n",
      "Rolling split months: ['2024-11', '2024-12', '2025-01', '2025-02', '2025-03', '2025-04', '2025-05', '2025-06', '2025-07', '2025-08']\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "df = load_master_csv(cfg.data_path)\n",
    "wide = build_monthly_wide(df)\n",
    "yearly_proxy = build_yearly_proxy(df, cfg.yearly_proxy_sources_priority)\n",
    "\n",
    "X_df, X_raw, y_who, mask_who, feature_cols = build_design_matrix(wide, cfg)\n",
    "\n",
    "rolling_split_months = pd.period_range(cfg.rolling_start, cfg.rolling_end, freq='M').strftime('%Y-%m').tolist()\n",
    "\n",
    "print('Total months:', len(X_df))\n",
    "print('WHO observed months:', int(mask_who.sum()))\n",
    "print('Feature count:', X_raw.shape[1])\n",
    "print('Rolling split months:', rolling_split_months)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696bc81",
   "metadata": {},
   "source": [
    "## 2) Train Step 2 (Joint Loss + Yearly Constraints + Early Stopping)\n",
    "\n",
    "- Only training period participates in optimization\n",
    "- WHO monthly + yearly proxy + L2 regularization\n",
    "- Early stopping to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "650a8a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_month</th>\n",
       "      <th>n_train_months</th>\n",
       "      <th>n_test_months</th>\n",
       "      <th>n_train_who</th>\n",
       "      <th>n_test_who</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>MAPE_train_%</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>MAPE_test_%</th>\n",
       "      <th>epochs_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11</td>\n",
       "      <td>47</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>15038.618123</td>\n",
       "      <td>111.260710</td>\n",
       "      <td>22145.363415</td>\n",
       "      <td>131.213332</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>1407.662540</td>\n",
       "      <td>15.817615</td>\n",
       "      <td>8219.554822</td>\n",
       "      <td>62.811584</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>3204.967181</td>\n",
       "      <td>33.121696</td>\n",
       "      <td>3428.580190</td>\n",
       "      <td>28.342706</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-02</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>5289.340329</td>\n",
       "      <td>70.335206</td>\n",
       "      <td>4148.724958</td>\n",
       "      <td>52.887248</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>6954.440550</td>\n",
       "      <td>92.213599</td>\n",
       "      <td>6476.264814</td>\n",
       "      <td>89.022920</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-04</td>\n",
       "      <td>52</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>8020.575480</td>\n",
       "      <td>90.006097</td>\n",
       "      <td>9410.384553</td>\n",
       "      <td>99.465835</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-05</td>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>8915.478915</td>\n",
       "      <td>83.349386</td>\n",
       "      <td>12205.320695</td>\n",
       "      <td>92.919870</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-06</td>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>9527.728566</td>\n",
       "      <td>77.639882</td>\n",
       "      <td>15000.813390</td>\n",
       "      <td>93.403158</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-07</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>11650.886116</td>\n",
       "      <td>83.813262</td>\n",
       "      <td>16544.241785</td>\n",
       "      <td>86.432967</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-08</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>12913.977157</td>\n",
       "      <td>91.442894</td>\n",
       "      <td>17482.099097</td>\n",
       "      <td>80.533522</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  split_month  n_train_months  n_test_months  n_train_who  n_test_who  \\\n",
       "0     2024-11              47             13           11          12   \n",
       "1     2024-12              48             12           12          11   \n",
       "2     2025-01              49             11           13          10   \n",
       "3     2025-02              50             10           14           9   \n",
       "4     2025-03              51              9           15           8   \n",
       "5     2025-04              52              8           16           7   \n",
       "6     2025-05              53              7           17           6   \n",
       "7     2025-06              54              6           18           5   \n",
       "8     2025-07              55              5           19           4   \n",
       "9     2025-08              56              4           20           3   \n",
       "\n",
       "     RMSE_train  MAPE_train_%     RMSE_test  MAPE_test_%  epochs_used  \n",
       "0  15038.618123    111.260710  22145.363415   131.213332         8000  \n",
       "1   1407.662540     15.817615   8219.554822    62.811584         8000  \n",
       "2   3204.967181     33.121696   3428.580190    28.342706         8000  \n",
       "3   5289.340329     70.335206   4148.724958    52.887248         8000  \n",
       "4   6954.440550     92.213599   6476.264814    89.022920         8000  \n",
       "5   8020.575480     90.006097   9410.384553    99.465835         8000  \n",
       "6   8915.478915     83.349386  12205.320695    92.919870         8000  \n",
       "7   9527.728566     77.639882  15000.813390    93.403158         8000  \n",
       "8  11650.886116     83.813262  16544.241785    86.432967         8000  \n",
       "9  12913.977157     91.442894  17482.099097    80.533522         8000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rolling train/test split evaluation: for each split_month in [rolling_start, rolling_end]\n",
    "outdir = Path(cfg.outdir)\n",
    "roll_dir = outdir / 'rolling_splits'\n",
    "ensure_outdir(str(roll_dir))\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Find intercept column index (should not be standardized)\n",
    "intercept_idx = feature_cols.index('intercept') if 'intercept' in feature_cols else None\n",
    "skip_cols = [intercept_idx] if intercept_idx is not None else []\n",
    "\n",
    "for split_month in rolling_split_months:\n",
    "    train_mask, test_mask = time_series_split_mask(X_df.index, split_month)\n",
    "\n",
    "    if int(train_mask.sum()) == 0 or int(test_mask.sum()) == 0:\n",
    "        continue\n",
    "\n",
    "    # Standardize features (using training set statistics only, skip intercept)\n",
    "    if cfg.standardize_features:\n",
    "        X_proc, feat_mean, feat_std = standardize_features(X_raw, train_mask, skip_cols=skip_cols)\n",
    "    else:\n",
    "        X_proc = X_raw.copy()\n",
    "        feat_mean = None\n",
    "        feat_std = None\n",
    "\n",
    "    # Year constraints (only full years entirely in training window)\n",
    "    year_constraints = build_year_constraints(X_df.index, yearly_proxy, train_mask)\n",
    "\n",
    "    # Copy cfg for this split (optionally override training budget for rolling)\n",
    "    cfg_i = copy.deepcopy(cfg)\n",
    "    cfg_i.split_month = split_month\n",
    "    cfg_i.epochs = int(getattr(cfg, 'rolling_max_epochs', cfg.epochs))\n",
    "    cfg_i.patience = int(getattr(cfg, 'rolling_patience', cfg.patience))\n",
    "\n",
    "    beta_hat, loss_df = train_step2_joint_loss(\n",
    "        X=X_proc,\n",
    "        y_who=y_who,\n",
    "        mask_who=mask_who,\n",
    "        year_constraints=year_constraints,\n",
    "        train_mask=train_mask,\n",
    "        cfg=cfg_i,\n",
    "        feature_cols=feature_cols,\n",
    "    )\n",
    "\n",
    "    # Predict for all months\n",
    "    x_pred = make_predictions(X_proc, beta_hat, cfg_i)\n",
    "\n",
    "    # Evaluation (only on months with WHO observations)\n",
    "    mask_train_who = train_mask & mask_who\n",
    "    mask_test_who = test_mask & mask_who\n",
    "\n",
    "    rmse_train = compute_rmse(y_who[mask_train_who], x_pred[mask_train_who]) if mask_train_who.any() else np.nan\n",
    "    mape_train = safe_mape(y_who[mask_train_who], x_pred[mask_train_who]) if mask_train_who.any() else np.nan\n",
    "    rmse_test = compute_rmse(y_who[mask_test_who], x_pred[mask_test_who]) if mask_test_who.any() else np.nan\n",
    "    mape_test = safe_mape(y_who[mask_test_who], x_pred[mask_test_who]) if mask_test_who.any() else np.nan\n",
    "\n",
    "    rows.append({\n",
    "        'split_month': split_month,\n",
    "        'n_train_months': int(train_mask.sum()),\n",
    "        'n_test_months': int(test_mask.sum()),\n",
    "        'n_train_who': int(mask_train_who.sum()),\n",
    "        'n_test_who': int(mask_test_who.sum()),\n",
    "        'RMSE_train': rmse_train,\n",
    "        'MAPE_train_%': mape_train,\n",
    "        'RMSE_test': rmse_test,\n",
    "        'MAPE_test_%': mape_test,\n",
    "        'epochs_used': int(loss_df['epoch'].max()) if not loss_df.empty and 'epoch' in loss_df.columns else np.nan,\n",
    "    })\n",
    "\n",
    "    # Plot: prediction vs WHO (with split line)\n",
    "    fname = f'who_vs_pred_step2_split_{split_month}.png'\n",
    "    title = f'WHO vs Predicted (Step 2) â€” split_month={split_month}'\n",
    "    plot_who_vs_pred(X_df.index, y_who, x_pred, train_mask, roll_dir, fname=fname, title=title)\n",
    "\n",
    "rolling_metrics_df = pd.DataFrame(rows).sort_values('split_month')\n",
    "rolling_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5363918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rolling plots to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_jointloss_wiki/rolling_splits\n",
      "Saved metrics to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_jointloss_wiki/rolling_splits/rolling_split_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Save rolling metrics table\n",
    "if 'rolling_metrics_df' in globals() and len(rolling_metrics_df) > 0:\n",
    "    rolling_metrics_df.to_csv(roll_dir / 'rolling_split_metrics.csv', index=False)\n",
    "    print('Saved rolling plots to:', roll_dir.resolve())\n",
    "    print('Saved metrics to:', (roll_dir / 'rolling_split_metrics.csv').resolve())\n",
    "else:\n",
    "    print('No rolling metrics produced. Check rolling_split_months and data coverage.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94bd02e",
   "metadata": {},
   "source": [
    "## 3) Plot and Save Outputs\n",
    "\n",
    "Includes:\n",
    "- Training loss curve\n",
    "- WHO vs predictions (with train/test split line)\n",
    "- Yearly aggregation vs OpenDengue\n",
    "- Step1 vs Step2 comparison\n",
    "- Parameter and prediction tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c506c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: keep the original single-split outputs if you still want them.\n",
    "# Set cfg.split_month to one of the rolling months (or any month) and re-run the original pipeline.\n",
    "#\n",
    "# Example:\n",
    "#   cfg.split_month = '2025-04'\n",
    "#   train_mask, test_mask = time_series_split_mask(X_df.index, cfg.split_month)\n",
    "#   ... (then follow the original cells 8-11 logic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
