{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f629dbf",
   "metadata": {},
   "source": [
    "# Step 2 Dengue Nowcasting (ARX + Joint Loss)\n",
    "\n",
    "This notebook implements Step 2 according to the requirements:\n",
    "- Training set: months **<= split_month** (rolling splits)\n",
    "- Test set: months **> split_month** (for evaluation only)\n",
    "- Joint loss: WHO monthly supervision + OpenDengue yearly constraints + L2 regularization\n",
    "- Use Step 1 estimates to impute missing WHO observations as $\\tilde{x}$ for lagged inputs\n",
    "- Output complete parameters, training curves, predictions, and plots\n",
    "\n",
    "Overfitting control: L2 regularization + early stopping + train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae4f6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install dependencies in your environment:\n",
    "# pip install -U pandas numpy torch matplotlib\n",
    "\n",
    "# Prevent kernel crashes from OpenMP/MKL library conflicts\n",
    "import os\n",
    "os.environ.setdefault('KMP_DUPLICATE_LIB_OK', 'TRUE')\n",
    "os.environ.setdefault('OMP_NUM_THREADS', '1')\n",
    "os.environ.setdefault('MKL_NUM_THREADS', '1')\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Further limit thread count\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1261ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Step2Config(data_path='master_data.csv', step1_predictions_path='outputs_step1_wiki/predictions_step1_monthly.csv', step1_pred_col='x_pred', start_month='2021-01', rolling_start='2024-11', rolling_end='2025-08', rolling_max_epochs=8000, rolling_patience=400, target_source='WHO', google_sources=('Google_Trends_Dengue_fever', 'Google_Trends_Dengue_vaccine'), use_wiki=True, wiki_path='total_dengue_views.csv', wiki_month_col='Month', wiki_value_col='Total_Views', wiki_transform='log1p', lags_y=(1, 2, 12), use_month_dummies=False, seasonal_period=12, use_fourier=True, fourier_K=2, yearly_proxy_sources_priority=('OpenDengue_State_Aggregated', 'OpenDengue_National_Yearly'), lambda_who=5.0, lambda_year=5.0, lambda_reg=0.0001, lambda_lag_reg=0.005, tune_n_splits=3, inner_val_n_who_months=3, tune_lambda_year_grid=(0.0, 0.03, 0.1, 0.3, 1.0, 3.0, 10.0), tune_lambda_reg_grid=(1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1), tune_epochs=6000, tune_patience=300, year_mape_tolerance=0.1, alpha_year_score=0.5, epochs=30000, lr=0.003, seed=42, target_scale=1000.0, early_stop=True, patience=1000, min_delta=1e-07, standardize_features=True, outdir='outputs_step2_period', clip_nonnegative=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Step2Config:\n",
    "    # Inputs\n",
    "    data_path: str = 'master_data.csv'\n",
    "    step1_predictions_path: str = os.path.join('outputs_step1_wiki', 'predictions_step1_monthly.csv')\n",
    "    step1_pred_col: str = 'x_pred'\n",
    "\n",
    "    # Modeling window\n",
    "    start_month: str = '2021-01'\n",
    "\n",
    "    # Rolling split evaluation (inclusive)\n",
    "    rolling_start: str = '2024-11'\n",
    "    rolling_end: str = '2025-08'\n",
    "    # To keep rolling evaluation runtime reasonable, you can override training budget per split\n",
    "    rolling_max_epochs: int = 8000\n",
    "    rolling_patience: int = 400\n",
    "\n",
    "    # Target/source names in master_data.csv\n",
    "    target_source: str = 'WHO'\n",
    "\n",
    "    # Exogenous signals\n",
    "    google_sources: Tuple[str, str] = (\n",
    "        'Google_Trends_Dengue_fever',\n",
    "        'Google_Trends_Dengue_vaccine',\n",
    "    )\n",
    "\n",
    "\n",
    "    # Wikipedia pageviews (external regressor)\n",
    "    use_wiki: bool = True\n",
    "    wiki_path: str = 'total_dengue_views.csv'\n",
    "    wiki_month_col: str = 'Month'\n",
    "    wiki_value_col: str = 'Total_Views'\n",
    "    wiki_transform: str = 'log1p'  # 'log1p' or 'none'\n",
    "\n",
    "    # Feature engineering\n",
    "    lags_y: Tuple[int, ...] = (1, 2, 12)  # include seasonal lag 12 for monthly data\n",
    "    use_month_dummies: bool = False\n",
    "\n",
    "    # Seasonality (periodic time series)\n",
    "    seasonal_period: int = 12  # monthly seasonality\n",
    "    use_fourier: bool = True  # set True to use Fourier terms instead of month dummies\n",
    "    fourier_K: int = 2         # number of Fourier harmonics\n",
    "    # OpenDengue yearly proxy sources (priority)\n",
    "    yearly_proxy_sources_priority: Tuple[str, ...] = (\n",
    "        'OpenDengue_State_Aggregated',\n",
    "        'OpenDengue_National_Yearly',\n",
    "    )\n",
    "\n",
    "    # Loss weights - Conservative: balance all three components\n",
    "    lambda_who: float = 5.0  # Moderate WHO weight\n",
    "    lambda_year: float = 5.0  # Strong yearly constraint to anchor predictions\n",
    "    lambda_reg: float = 1e-4  # Base regularization\n",
    "    lambda_lag_reg: float = 5e-3  # Moderate lag regularization\n",
    "\n",
    "    # Lambda tuning (nested on early splits -> pick one global lambda set)\n",
    "    tune_n_splits: int = 3               # how many early rolling splits to tune on\n",
    "    inner_val_n_who_months: int = 3      # validation months (picked from last WHO-observed months in the train window)\n",
    "    tune_lambda_year_grid: Tuple[float, ...] = (0.0, 0.03, 0.1, 0.3, 1.0, 3.0, 10.0)\n",
    "    tune_lambda_reg_grid: Tuple[float, ...] = (1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1)\n",
    "    tune_epochs: int = 6000\n",
    "    tune_patience: int = 300\n",
    "    year_mape_tolerance: float = 0.10    # 10% average yearly MAPE tolerance (constraint-style selection)\n",
    "    alpha_year_score: float = 0.5        # fallback tradeoff weight if no candidate meets the tolerance\n",
    "\n",
    "    # Optimization\n",
    "    epochs: int = 30000  # Sufficient epochs for convergence\n",
    "    lr: float = 3e-3  # Slower learning rate for stability\n",
    "    seed: int = 42\n",
    "    target_scale: float = 1000.0\n",
    "\n",
    "    # Overfitting controls\n",
    "    early_stop: bool = True\n",
    "    patience: int = 1000  # Large patience for stable convergence\n",
    "    min_delta: float = 1e-7  # Smaller threshold for early stopping\n",
    "\n",
    "    # Scaling features (standardize using train stats)\n",
    "    standardize_features: bool = True\n",
    "\n",
    "    # Output\n",
    "    outdir: str = 'outputs_step2_period_lambda_tuning'\n",
    "    clip_nonnegative: bool = True\n",
    "\n",
    "\n",
    "cfg = Step2Config()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e6b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_outdir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def parse_monthly_date_any(s: pd.Series) -> pd.Series:\n",
    "    x = s.astype(str)\n",
    "    is_ym = x.str.match('^[0-9]{4}-[0-9]{2}$')\n",
    "    x = np.where(is_ym, x + '-01', x)\n",
    "    dt = pd.to_datetime(x, errors='coerce')\n",
    "    return dt\n",
    "\n",
    "\n",
    "def load_master_csv(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    expected = {'resolution', 'date', 'value', 'source'}\n",
    "    missing = expected - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"master_data.csv missing columns: {sorted(missing)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_monthly_wide(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    m = df[df['resolution'].astype(str).str.lower().eq('monthly')].copy()\n",
    "    m['date'] = parse_monthly_date_any(m['date'])\n",
    "    m = m.dropna(subset=['date']).copy()\n",
    "    wide = (\n",
    "        m.pivot_table(index='date', columns='source', values='value', aggfunc='mean')\n",
    "        .sort_index()\n",
    "    )\n",
    "    return wide\n",
    "\n",
    "\n",
    "def build_yearly_proxy(df: pd.DataFrame, priority_sources: Tuple[str, ...]) -> pd.DataFrame:\n",
    "    y = df[df['resolution'].astype(str).str.lower().eq('yearly')].copy()\n",
    "    y['year'] = pd.to_numeric(y['date'], errors='coerce').astype('Int64')\n",
    "    y = y.dropna(subset=['year'])\n",
    "    y['year'] = y['year'].astype(int)\n",
    "\n",
    "    pivot = (\n",
    "        y.pivot_table(index='year', columns='source', values='value', aggfunc='mean')\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    chosen = []\n",
    "    for year, row in pivot.iterrows():\n",
    "        val = np.nan\n",
    "        src = None\n",
    "        for s in priority_sources:\n",
    "            if s in row.index and pd.notna(row[s]):\n",
    "                val = float(row[s])\n",
    "                src = s\n",
    "                break\n",
    "        if pd.notna(val):\n",
    "            chosen.append((year, val, src))\n",
    "\n",
    "    return pd.DataFrame(chosen, columns=['year', 'od_total', 'od_source'])\n",
    "\n",
    "\n",
    "def load_step1_predictions(cfg: Step2Config) -> pd.Series:\n",
    "    if not os.path.exists(cfg.step1_predictions_path):\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    s1 = pd.read_csv(cfg.step1_predictions_path)\n",
    "    if 'date' not in s1.columns:\n",
    "        raise ValueError(f\"Step 1 predictions file missing 'date': {cfg.step1_predictions_path}\")\n",
    "\n",
    "    s1['date'] = parse_monthly_date_any(s1['date'])\n",
    "    s1 = s1.dropna(subset=['date']).copy()\n",
    "    s1 = s1.sort_values('date')\n",
    "\n",
    "    col = cfg.step1_pred_col if cfg.step1_pred_col in s1.columns else None\n",
    "    if col is None:\n",
    "        cand = [c for c in s1.columns if 'pred' in c.lower()]\n",
    "        if not cand:\n",
    "            cand = [c for c in s1.columns if c != 'date']\n",
    "        if not cand:\n",
    "            raise ValueError('No prediction column found in Step 1 predictions file.')\n",
    "        col = cand[0]\n",
    "\n",
    "    ser = s1.set_index('date')[col].astype(float)\n",
    "    return ser\n",
    "\n",
    "\n",
    "\n",
    "def load_wiki_monthly(cfg: Step2Config) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Load monthly Wikipedia pageviews time series.\n",
    "    Expected columns: cfg.wiki_month_col (YYYY-MM) and cfg.wiki_value_col (numeric)\n",
    "    Returns: pd.Series indexed by month (Timestamp at month start).\n",
    "    \"\"\"\n",
    "    if (not getattr(cfg, 'use_wiki', False)) or (not os.path.exists(cfg.wiki_path)):\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    w = pd.read_csv(cfg.wiki_path)\n",
    "    if cfg.wiki_month_col not in w.columns or cfg.wiki_value_col not in w.columns:\n",
    "        raise ValueError(\n",
    "            f\"Wiki file must contain columns '{cfg.wiki_month_col}' and '{cfg.wiki_value_col}'. Got: {list(w.columns)}\"\n",
    "        )\n",
    "\n",
    "    w = w.copy()\n",
    "    w['date'] = parse_monthly_date_any(w[cfg.wiki_month_col])\n",
    "    w = w.dropna(subset=['date']).sort_values('date')\n",
    "    s = pd.to_numeric(w[cfg.wiki_value_col], errors='coerce')\n",
    "    ser = pd.Series(s.values, index=w['date']).groupby(level=0).mean()\n",
    "    return ser\n",
    "\n",
    "def add_month_dummies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dt = pd.to_datetime(df.index)\n",
    "    m = pd.get_dummies(dt.month, prefix='m', drop_first=True)\n",
    "    m.index = df.index\n",
    "    return pd.concat([df, m.astype(float)], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def add_fourier_features(df: pd.DataFrame, period: int = 12, K: int = 2, prefix: str = 's') -> pd.DataFrame:\n",
    "    \"\"\"Add Fourier seasonality terms based on month-of-year.\"\"\"\n",
    "    dt = pd.to_datetime(df.index)\n",
    "    m = dt.month.astype(float)\n",
    "    out = df.copy()\n",
    "    for k in range(1, int(K) + 1):\n",
    "        out[f'{prefix}_sin{k}'] = np.sin(2 * np.pi * k * m / float(period))\n",
    "        out[f'{prefix}_cos{k}'] = np.cos(2 * np.pi * k * m / float(period))\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def build_design_matrix(wide: pd.DataFrame, cfg: Step2Config) -> tuple[pd.DataFrame, np.ndarray, np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Build design matrix.\n",
    "    Critical fix: lagged variables x_tilde must be scaled by target_scale to match target scale.\n",
    "    \"\"\"\n",
    "    start_dt = pd.to_datetime(cfg.start_month + '-01')\n",
    "    wide = wide.loc[wide.index >= start_dt].copy()\n",
    "\n",
    "    target = cfg.target_source\n",
    "    if target not in wide.columns:\n",
    "        wide[target] = np.nan\n",
    "\n",
    "    g1, g2 = cfg.google_sources\n",
    "    for g in [g1, g2]:\n",
    "        if g not in wide.columns:\n",
    "            raise ValueError(f\"Missing Google source '{g}' in monthly data.\")\n",
    "\n",
    "    y_who = pd.to_numeric(wide[target], errors='coerce')\n",
    "\n",
    "    step1_ser = load_step1_predictions(cfg)\n",
    "    wide['STEP1_est'] = step1_ser.reindex(wide.index)\n",
    "\n",
    "    # x_tilde: WHO when observed, else Step1 estimate (for lagged inputs)\n",
    "    x_tilde = y_who.where(y_who.notna(), wide['STEP1_est'])\n",
    "    \n",
    "    # Critical fix: scale lagged variables to match target scale\n",
    "    x_tilde_scaled = x_tilde / cfg.target_scale\n",
    "\n",
    "    X_df = pd.DataFrame(index=wide.index)\n",
    "    \n",
    "    # Normalize Google Trends from [0, 100] to [0, 1]\n",
    "    X_df['g_fever'] = pd.to_numeric(wide[g1], errors='coerce') / 100.0\n",
    "    X_df['g_vaccine'] = pd.to_numeric(wide[g2], errors='coerce') / 100.0\n",
    "\n",
    "\n",
    "    # Wikipedia pageviews regressor\n",
    "    if getattr(cfg, 'use_wiki', False):\n",
    "        wiki_ser = load_wiki_monthly(cfg)\n",
    "        wide['WIKI_raw'] = wiki_ser.reindex(wide.index)\n",
    "        wiki_raw = pd.to_numeric(wide['WIKI_raw'], errors='coerce')\n",
    "        wiki_raw = wiki_raw.interpolate(limit_direction='both').ffill().bfill()\n",
    "        if str(getattr(cfg, 'wiki_transform', 'log1p')).lower() == 'log1p':\n",
    "            X_df['wiki_views'] = np.log1p(wiki_raw.astype(float))\n",
    "        else:\n",
    "            X_df['wiki_views'] = wiki_raw.astype(float)\n",
    "\n",
    "    # Use scaled values for lagged variables\n",
    "    for k in cfg.lags_y:\n",
    "        X_df[f'x_tilde_lag{k}'] = x_tilde_scaled.shift(k)\n",
    "\n",
    "    # Seasonality features (periodic time series)\n",
    "    # Prefer Fourier terms when enabled; otherwise fall back to month dummies.\n",
    "    if getattr(cfg, 'use_fourier', False):\n",
    "        X_df = add_fourier_features(\n",
    "            X_df,\n",
    "            period=int(getattr(cfg, 'seasonal_period', 12)),\n",
    "            K=int(getattr(cfg, 'fourier_K', 2)),\n",
    "            prefix='s'\n",
    "        )\n",
    "    elif cfg.use_month_dummies:\n",
    "        X_df = add_month_dummies(X_df)\n",
    "\n",
    "\n",
    "    X_df['intercept'] = 1.0\n",
    "\n",
    "    # Light imputation on exogenous features only (lags can remain NaN and be imputed later)\n",
    "    exo_cols = ['g_fever', 'g_vaccine'] + (['wiki_views'] if 'wiki_views' in X_df.columns else [])\n",
    "    for col in exo_cols:\n",
    "        X_df[col] = X_df[col].interpolate(limit_direction='both').ffill().bfill()\n",
    "\n",
    "    feature_cols = [c for c in X_df.columns if c != 'intercept'] + ['intercept']\n",
    "    X = X_df[feature_cols].to_numpy(dtype=np.float32)\n",
    "    y = y_who.to_numpy(dtype=np.float32)\n",
    "    mask_who = np.isfinite(y)\n",
    "\n",
    "    return X_df, X, y, mask_who, feature_cols\n",
    "\n",
    "\n",
    "def build_year_constraints(dates: pd.DatetimeIndex, yearly_proxy: pd.DataFrame, train_mask: np.ndarray) -> List[tuple]:\n",
    "    df_dates = pd.DataFrame({'date': pd.to_datetime(dates)})\n",
    "    df_dates['year'] = df_dates['date'].dt.year\n",
    "    df_dates['month'] = df_dates['date'].dt.month\n",
    "\n",
    "    constraints = []\n",
    "    for _, r in yearly_proxy.iterrows():\n",
    "        y = int(r['year'])\n",
    "        od_total = float(r['od_total'])\n",
    "        od_src = str(r['od_source'])\n",
    "\n",
    "        idx = df_dates.index[df_dates['year'].eq(y)].to_numpy()\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "\n",
    "        # require all 12 months and all months in training window\n",
    "        months_present = set(df_dates.loc[idx, 'month'].tolist())\n",
    "        if months_present != set(range(1, 13)):\n",
    "            continue\n",
    "        if not np.all(train_mask[idx]):\n",
    "            continue\n",
    "\n",
    "        constraints.append((y, idx, od_total, od_src))\n",
    "\n",
    "    return constraints\n",
    "\n",
    "\n",
    "def time_series_split_mask(index: pd.DatetimeIndex, split_month: str):\n",
    "    split_dt = pd.to_datetime(split_month + '-01')\n",
    "    train_mask = index <= split_dt\n",
    "    test_mask = index > split_dt\n",
    "    return train_mask, test_mask\n",
    "\n",
    "\n",
    "def standardize_features(X: np.ndarray, train_mask: np.ndarray, skip_cols: List[int] = None):\n",
    "    \"\"\"\n",
    "    Standardize feature matrix.\n",
    "    skip_cols: column indices to skip standardization (e.g., intercept term).\n",
    "    \"\"\"\n",
    "    X2 = X.copy()\n",
    "    n, p = X2.shape\n",
    "    means = np.zeros(p, dtype=np.float32)\n",
    "    stds = np.ones(p, dtype=np.float32)\n",
    "    \n",
    "    if skip_cols is None:\n",
    "        skip_cols = []\n",
    "\n",
    "    for j in range(p):\n",
    "        col = X2[:, j]\n",
    "        col_train = col[train_mask]\n",
    "        \n",
    "        # For intercept or specified columns, skip standardization\n",
    "        if j in skip_cols:\n",
    "            # Only impute NaN\n",
    "            col = np.where(np.isfinite(col), col, 0.0)\n",
    "            X2[:, j] = col\n",
    "            continue\n",
    "        \n",
    "        m = np.nanmean(col_train)\n",
    "        s = np.nanstd(col_train)\n",
    "        if not np.isfinite(m):\n",
    "            m = 0.0\n",
    "        if (not np.isfinite(s)) or s <= 1e-12:\n",
    "            s = 1.0\n",
    "        means[j] = m\n",
    "        stds[j] = s\n",
    "\n",
    "        # impute NaNs with train mean, then standardize\n",
    "        col = np.where(np.isfinite(col), col, m)\n",
    "        X2[:, j] = (col - m) / s\n",
    "\n",
    "    return X2, means, stds\n",
    "\n",
    "\n",
    "def train_step2_joint_loss(\n",
    "    X: np.ndarray,\n",
    "    y_who: np.ndarray,\n",
    "    mask_who: np.ndarray,\n",
    "    year_constraints: List[tuple],\n",
    "    train_mask: np.ndarray,\n",
    "    cfg: Step2Config,\n",
    "    feature_cols: List[str],\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train with differentiated regularization: stronger penalty on lag coefficients.\n",
    "    \"\"\"\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    y_scaled = y_who / cfg.target_scale\n",
    "    y_t = torch.tensor(y_scaled, dtype=torch.float32, device=device)\n",
    "    mask_who_t = torch.tensor(mask_who & train_mask, dtype=torch.bool, device=device)\n",
    "\n",
    "    year_terms = []\n",
    "    for (year, idx, od_total, od_src) in year_constraints:\n",
    "        year_terms.append(\n",
    "            (year, torch.tensor(idx, dtype=torch.long, device=device), float(od_total / cfg.target_scale), od_src)\n",
    "        )\n",
    "\n",
    "    # Identify lag feature indices for differentiated regularization\n",
    "    lag_indices = [i for i, col in enumerate(feature_cols) if 'lag' in col.lower()]\n",
    "\n",
    "    p = X_t.shape[1]\n",
    "    beta = torch.nn.Parameter(torch.zeros(p, dtype=torch.float32, device=device))\n",
    "    opt = torch.optim.Adam([beta], lr=cfg.lr)\n",
    "\n",
    "    rows = []\n",
    "    best_loss = np.inf\n",
    "    best_beta = None\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        x = X_t @ beta\n",
    "\n",
    "        if mask_who_t.any():\n",
    "            diff_who = x[mask_who_t] - y_t[mask_who_t]\n",
    "            L_who = (diff_who ** 2).mean()\n",
    "        else:\n",
    "            L_who = torch.tensor(0.0, device=device)\n",
    "\n",
    "        if len(year_terms) > 0:\n",
    "            diffs = []\n",
    "            for _, idx_t, od_total_scaled, _ in year_terms:\n",
    "                year_sum = x.index_select(0, idx_t).sum()\n",
    "                diffs.append((year_sum - od_total_scaled) ** 2)\n",
    "            L_year = torch.stack(diffs).mean()\n",
    "        else:\n",
    "            L_year = torch.tensor(0.0, device=device)\n",
    "\n",
    "        # Differentiated regularization: base + extra penalty on lag coefficients\n",
    "        L_reg_base = (beta ** 2).sum()\n",
    "        if lag_indices:\n",
    "            beta_lag = beta[lag_indices]\n",
    "            L_reg_lag = (beta_lag ** 2).sum()\n",
    "        else:\n",
    "            L_reg_lag = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        L_reg = L_reg_base\n",
    "        L_total = cfg.lambda_who * L_who + cfg.lambda_year * L_year + cfg.lambda_reg * L_reg + cfg.lambda_lag_reg * L_reg_lag\n",
    "        L_total.backward()\n",
    "        opt.step()\n",
    "\n",
    "        val = float(L_total.detach().cpu().item())\n",
    "        rows.append({\n",
    "            'epoch': epoch,\n",
    "            'L_total': val,\n",
    "            'L_who': float(L_who.detach().cpu().item()),\n",
    "            'L_year': float(L_year.detach().cpu().item()),\n",
    "            'L_reg': float(L_reg.detach().cpu().item()),\n",
    "            'L_lag_reg': float(L_reg_lag.detach().cpu().item()) if lag_indices else 0.0,\n",
    "        })\n",
    "\n",
    "        if not np.isfinite(val):\n",
    "            raise RuntimeError('Training diverged (loss is NaN/Inf).')\n",
    "\n",
    "        if cfg.early_stop:\n",
    "            if val < best_loss - cfg.min_delta:\n",
    "                best_loss = val\n",
    "                best_beta = beta.detach().cpu().numpy().copy()\n",
    "                bad_epochs = 0\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "                if bad_epochs >= cfg.patience:\n",
    "                    break\n",
    "\n",
    "    beta_hat = beta.detach().cpu().numpy() if best_beta is None else best_beta\n",
    "    loss_df = pd.DataFrame(rows)\n",
    "    return beta_hat, loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d569c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.sqrt(np.nanmean((y_true - y_pred) ** 2)))\n",
    "\n",
    "\n",
    "def safe_mape(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = np.where(np.abs(y_true) < 1e-12, np.nan, np.abs(y_true))\n",
    "    return float(np.nanmean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_yearly_mape_from_constraints(y_pred: np.ndarray, year_constraints: list) -> float:\n",
    "    \"\"\"Average yearly MAPE over the provided year_constraints.\n",
    "    year_constraints entries: (year, idx_array, od_total, od_source)\n",
    "    \"\"\"\n",
    "    if year_constraints is None or len(year_constraints) == 0:\n",
    "        return float('nan')\n",
    "    errs = []\n",
    "    for (year, idx, od_total, _src) in year_constraints:\n",
    "        pred_total = float(np.nansum(y_pred[np.asarray(idx, dtype=int)]))\n",
    "        denom = od_total if abs(od_total) > 1e-12 else np.nan\n",
    "        errs.append(abs(pred_total - od_total) / denom)\n",
    "    return float(np.nanmean(errs) * 100.0)\n",
    "\n",
    "\n",
    "\n",
    "def make_predictions(X: np.ndarray, beta_hat: np.ndarray, cfg: Step2Config) -> np.ndarray:\n",
    "    x_scaled = X @ beta_hat\n",
    "    x = x_scaled * cfg.target_scale\n",
    "    if cfg.clip_nonnegative:\n",
    "        x = np.maximum(x, 0.0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def plot_loss_curve(loss_df: pd.DataFrame, outdir: Path) -> None:\n",
    "    fig = plt.figure()\n",
    "    plt.plot(loss_df['epoch'], loss_df['L_total'], label='Total')\n",
    "    if (loss_df['L_who'] != 0).any():\n",
    "        plt.plot(loss_df['epoch'], loss_df['L_who'], label='WHO')\n",
    "    if (loss_df['L_year'] != 0).any():\n",
    "        plt.plot(loss_df['epoch'], loss_df['L_year'], label='Yearly')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (log scale)')\n",
    "    plt.title('Training Loss Curve (Step 2)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outdir / 'loss_curve_step2.png', dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_who_vs_pred(dates: pd.DatetimeIndex, y_who: np.ndarray, x_pred: np.ndarray, train_mask: np.ndarray, outdir: Path, fname: str = 'who_vs_pred_step2.png', title: str = None) -> None:\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    plt.plot(dates, x_pred, label='Predicted (Step 2)')\n",
    "    plt.plot(dates, y_who, label='WHO observed')\n",
    "    split_dt = dates[train_mask].max() if train_mask.any() else None\n",
    "    if split_dt is not None:\n",
    "        plt.axvline(split_dt, linestyle='--', color='gray', label='Train/Test split')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Monthly dengue cases')\n",
    "    plt.title(title if title is not None else 'WHO vs Prediction (Step 2)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outdir / fname, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_yearly_vs_od(pred_df: pd.DataFrame, yearly_proxy: pd.DataFrame, outdir: Path) -> None:\n",
    "    pred_year = pred_df.groupby('year', as_index=False)['x_pred'].sum().rename(columns={'x_pred': 'pred_year_total'})\n",
    "    if yearly_proxy.empty:\n",
    "        return\n",
    "    merged = pred_year.merge(yearly_proxy, on='year', how='inner').sort_values('year')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(merged['year'], merged['pred_year_total'], marker='o', label='Predicted yearly sum')\n",
    "    plt.plot(merged['year'], merged['od_total'], marker='o', label='OpenDengue yearly total')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Total dengue cases (year)')\n",
    "    plt.title('Yearly Aggregation: Prediction vs OpenDengue (Step 2)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outdir / 'yearly_vs_opendengue_step2.png', dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    merged.to_csv(outdir / 'yearly_comparison_step2.csv', index=False)\n",
    "\n",
    "\n",
    "def plot_step1_vs_step2(pred_df: pd.DataFrame, outdir: Path):\n",
    "    if 'x_step1' not in pred_df.columns:\n",
    "        return\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    dates = pd.to_datetime(pred_df['date'] + '-01')\n",
    "    plt.plot(dates, pred_df['x_step1'], label='Step 1')\n",
    "    plt.plot(dates, pred_df['x_pred'], label='Step 2')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Monthly dengue cases')\n",
    "    plt.title('Step 1 vs Step 2 Predictions')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outdir / 'step1_vs_step2.png', dpi=200)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84fae24",
   "metadata": {},
   "source": [
    "## 1) Load Data, Build Features, and Split Train/Test\n",
    "\n",
    "- Use Step 1 predictions to impute missing WHO observations for $\\tilde{x}$\n",
    "- Build ARX features: Google Trends + lagged variables + month dummies\n",
    "- Training/Test split is evaluated over a rolling set of split months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d799a3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total months: 60\n",
      "WHO observed months: 23\n",
      "Feature count: 11\n",
      "Rolling split months: ['2024-11', '2024-12', '2025-01', '2025-02', '2025-03', '2025-04', '2025-05', '2025-06', '2025-07', '2025-08']\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "df = load_master_csv(cfg.data_path)\n",
    "wide = build_monthly_wide(df)\n",
    "yearly_proxy = build_yearly_proxy(df, cfg.yearly_proxy_sources_priority)\n",
    "\n",
    "X_df, X_raw, y_who, mask_who, feature_cols = build_design_matrix(wide, cfg)\n",
    "\n",
    "rolling_split_months = pd.period_range(cfg.rolling_start, cfg.rolling_end, freq='M').strftime('%Y-%m').tolist()\n",
    "\n",
    "print('Total months:', len(X_df))\n",
    "print('WHO observed months:', int(mask_who.sum()))\n",
    "print('Feature count:', X_raw.shape[1])\n",
    "print('Rolling split months:', rolling_split_months)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696bc81",
   "metadata": {},
   "source": [
    "## 2) Train Step 2 (Joint Loss + Yearly Constraints + Early Stopping)\n",
    "\n",
    "- Only training period participates in optimization\n",
    "- WHO monthly + yearly proxy + L2 regularization\n",
    "- Early stopping to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function, how $\\lambda$ is selected, and how to reduce loss\n",
    "\n",
    "### 1) Step 2 prediction model\n",
    "Step 2 is a linear ARX-style model (trained by gradient descent):\n",
    "\n",
    "$$\n",
    "\\hat y_t = \\text{target\\_scale}\\cdot (X_t\\beta)\\,,\n",
    "$$\n",
    "\n",
    "where **$X_t$** contains:\n",
    "- Step 1 monthly prediction (as a regressor)\n",
    "- lag features (e.g., $t-1, t-2, t-12$)\n",
    "- exogenous signals (Google Trends, and Wiki pageviews if enabled)\n",
    "- seasonality features: either **month dummies** (default) or **Fourier terms** (sin/cos)\n",
    "\n",
    "**Are months represented by trig functions?**  \n",
    "- If `cfg.use_fourier = True`, month seasonality uses **sin/cos Fourier terms** (triangular functions).\n",
    "- If `cfg.use_month_dummies = True` (default here), month seasonality uses **12 dummy variables** (no trig functions).\n",
    "\n",
    "### 2) Joint loss (what is being minimized)\n",
    "The training objective is:\n",
    "\n",
    "$$\n",
    "L(\\beta)=\\lambda_{who}L_{who} + \\lambda_{year}L_{year} + \\lambda_{reg}L_{reg} + \\lambda_{lag\\_reg}L_{lag\\_reg}\n",
    "$$\n",
    "\n",
    "- $L_{who}$: mean squared error on **WHO-observed months** in the training window  \n",
    "- $L_{year}$: mean squared error of **yearly sums** vs the OpenDengue yearly proxy (only for full years fully inside the training window)  \n",
    "- $L_{reg}$: $\\ell_2$ penalty on all coefficients  \n",
    "- $L_{lag\\_reg}$: extra $\\ell_2$ penalty on **lag coefficients** (to stabilize the autoregressive part)\n",
    "\n",
    "### 3) How $(\\lambda_{year},\\lambda_{reg})$ are chosen (nested on early splits)\n",
    "This notebook tunes $(\\lambda_{year},\\lambda_{reg})$ using a **nested** approach:\n",
    "\n",
    "1. Take the **first `cfg.tune_n_splits`** rolling split months as tuning splits.\n",
    "2. For each tuning split, create an **inner validation** set using the **last `cfg.inner_val_n_who_months` WHO-observed months** inside the training window.\n",
    "3. For each candidate $(\\lambda_{year},\\lambda_{reg})$ on a log-grid:\n",
    "   - Train on **train-inner** only (and build yearly constraints on train-inner only, to avoid leaking validation months through the yearly term).\n",
    "   - Evaluate **validation RMSE/MAPE** on the inner validation months.\n",
    "   - Track **yearly MAPE** on the constrained years (train-inner full years).\n",
    "4. Pick ONE **global** $(\\lambda_{year},\\lambda_{reg})$ and then **fix it for all rolling splits**:\n",
    "   - Prefer candidates that satisfy `year_MAPE_mean <= cfg.year_mape_tolerance` (constraint style), then minimize validation RMSE.\n",
    "   - If none satisfy the tolerance, minimize a fallback score:\n",
    "     $(\\text{valRMSE} + \\alpha\\cdot \\text{yearMAPE})$.\n",
    "\n",
    "### 4) Practical ways to reduce loss while still respecting yearly totals\n",
    "- **Increase `lambda_year`** if yearly totals drift (but watch for flattening peaks).\n",
    "- **Increase `lambda_reg`** if coefficients become unstable across splits (overfitting).\n",
    "- Keep **feature standardization on** (`cfg.standardize_features=True`) so `lambda_reg` behaves consistently.\n",
    "- Use **seasonal lag (12)** and (optionally) **Fourier** terms to reduce systematic seasonal error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "650a8a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning on early splits: ['2024-11', '2024-12', '2025-01']\n",
      "\n",
      "Selected GLOBAL lambdas:\n",
      "  lambda_year = 0.3\n",
      "  lambda_reg  = 0.01\n",
      "  selection_rule = constraint: year_MAPE_mean <= 10.0% then min val_RMSE\n",
      "Saved lambda tuning table to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits/lambda_tuning_table.csv\n",
      "Saved rolling plots to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits\n",
      "Saved metrics to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits/rolling_split_metrics.csv\n",
      "Saved coefficients (long) to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits/rolling_split_coefficients_long.csv\n",
      "Saved coefficients (wide/raw) to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits/rolling_split_coefficients_raw_wide.csv\n",
      "Saved coefficients (wide/std) to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits/rolling_split_coefficients_std_wide.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_month</th>\n",
       "      <th>n_train_months</th>\n",
       "      <th>n_test_months</th>\n",
       "      <th>n_train_who</th>\n",
       "      <th>n_test_who</th>\n",
       "      <th>n_year_constraints</th>\n",
       "      <th>year_constraint_years</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>MAPE_train_%</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>MAPE_test_%</th>\n",
       "      <th>RMSE_test_seasonal_naive</th>\n",
       "      <th>MAPE_test_seasonal_naive_%</th>\n",
       "      <th>epochs_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11</td>\n",
       "      <td>47</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2021;2022;2023</td>\n",
       "      <td>4746.507080</td>\n",
       "      <td>49.405448</td>\n",
       "      <td>8350.712235</td>\n",
       "      <td>77.022830</td>\n",
       "      <td>13004.736797</td>\n",
       "      <td>63.822491</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3040.842630</td>\n",
       "      <td>18.874380</td>\n",
       "      <td>3995.780801</td>\n",
       "      <td>49.530049</td>\n",
       "      <td>13582.735466</td>\n",
       "      <td>69.476023</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3054.275418</td>\n",
       "      <td>23.156339</td>\n",
       "      <td>4598.132580</td>\n",
       "      <td>52.140318</td>\n",
       "      <td>14245.188974</td>\n",
       "      <td>75.617242</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-02</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3110.339104</td>\n",
       "      <td>27.174718</td>\n",
       "      <td>5015.811989</td>\n",
       "      <td>48.352764</td>\n",
       "      <td>15005.998652</td>\n",
       "      <td>79.281025</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3154.317711</td>\n",
       "      <td>31.964091</td>\n",
       "      <td>5439.884321</td>\n",
       "      <td>43.157386</td>\n",
       "      <td>15899.267121</td>\n",
       "      <td>81.798489</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-04</td>\n",
       "      <td>52</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3135.910229</td>\n",
       "      <td>36.841176</td>\n",
       "      <td>5828.491255</td>\n",
       "      <td>34.216595</td>\n",
       "      <td>16995.299333</td>\n",
       "      <td>90.087162</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-05</td>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3086.681707</td>\n",
       "      <td>37.353337</td>\n",
       "      <td>6320.739998</td>\n",
       "      <td>32.132624</td>\n",
       "      <td>18348.386646</td>\n",
       "      <td>98.998839</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-06</td>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3072.556672</td>\n",
       "      <td>38.582210</td>\n",
       "      <td>6518.334260</td>\n",
       "      <td>25.231477</td>\n",
       "      <td>20094.268611</td>\n",
       "      <td>115.578294</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-07</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>2999.191523</td>\n",
       "      <td>37.220671</td>\n",
       "      <td>6641.698030</td>\n",
       "      <td>22.921137</td>\n",
       "      <td>20718.814071</td>\n",
       "      <td>102.313578</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-08</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>2955.490885</td>\n",
       "      <td>36.552826</td>\n",
       "      <td>7982.241258</td>\n",
       "      <td>30.313746</td>\n",
       "      <td>21525.254942</td>\n",
       "      <td>94.270769</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  split_month  n_train_months  n_test_months  n_train_who  n_test_who  \\\n",
       "0     2024-11              47             13           11          12   \n",
       "1     2024-12              48             12           12          11   \n",
       "2     2025-01              49             11           13          10   \n",
       "3     2025-02              50             10           14           9   \n",
       "4     2025-03              51              9           15           8   \n",
       "5     2025-04              52              8           16           7   \n",
       "6     2025-05              53              7           17           6   \n",
       "7     2025-06              54              6           18           5   \n",
       "8     2025-07              55              5           19           4   \n",
       "9     2025-08              56              4           20           3   \n",
       "\n",
       "   n_year_constraints year_constraint_years   RMSE_train  MAPE_train_%  \\\n",
       "0                   3        2021;2022;2023  4746.507080     49.405448   \n",
       "1                   4   2021;2022;2023;2024  3040.842630     18.874380   \n",
       "2                   4   2021;2022;2023;2024  3054.275418     23.156339   \n",
       "3                   4   2021;2022;2023;2024  3110.339104     27.174718   \n",
       "4                   4   2021;2022;2023;2024  3154.317711     31.964091   \n",
       "5                   4   2021;2022;2023;2024  3135.910229     36.841176   \n",
       "6                   4   2021;2022;2023;2024  3086.681707     37.353337   \n",
       "7                   4   2021;2022;2023;2024  3072.556672     38.582210   \n",
       "8                   4   2021;2022;2023;2024  2999.191523     37.220671   \n",
       "9                   4   2021;2022;2023;2024  2955.490885     36.552826   \n",
       "\n",
       "     RMSE_test  MAPE_test_%  RMSE_test_seasonal_naive  \\\n",
       "0  8350.712235    77.022830              13004.736797   \n",
       "1  3995.780801    49.530049              13582.735466   \n",
       "2  4598.132580    52.140318              14245.188974   \n",
       "3  5015.811989    48.352764              15005.998652   \n",
       "4  5439.884321    43.157386              15899.267121   \n",
       "5  5828.491255    34.216595              16995.299333   \n",
       "6  6320.739998    32.132624              18348.386646   \n",
       "7  6518.334260    25.231477              20094.268611   \n",
       "8  6641.698030    22.921137              20718.814071   \n",
       "9  7982.241258    30.313746              21525.254942   \n",
       "\n",
       "   MAPE_test_seasonal_naive_%  epochs_used  \n",
       "0                   63.822491         8000  \n",
       "1                   69.476023         8000  \n",
       "2                   75.617242         8000  \n",
       "3                   79.281025         8000  \n",
       "4                   81.798489         8000  \n",
       "5                   90.087162         8000  \n",
       "6                   98.998839         8000  \n",
       "7                  115.578294         8000  \n",
       "8                  102.313578         8000  \n",
       "9                   94.270769         8000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rolling train/test split evaluation: for each split_month in [rolling_start, rolling_end]\n",
    "outdir = Path(cfg.outdir)\n",
    "roll_dir = outdir / 'rolling_splits'\n",
    "ensure_outdir(str(roll_dir))\n",
    "\n",
    "metric_rows = []\n",
    "coef_rows = []\n",
    "\n",
    "# Find intercept column index (should not be standardized)\n",
    "intercept_idx = feature_cols.index('intercept') if 'intercept' in feature_cols else None\n",
    "skip_cols = [intercept_idx] if intercept_idx is not None else []\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# (Nested) lambda tuning on early splits -> pick ONE global (lambda_year, lambda_reg)\n",
    "# -------------------------\n",
    "def make_inner_train_val_masks(train_mask_full: np.ndarray, mask_who: np.ndarray, n_val_who_months: int):\n",
    "    \"\"\"Pick validation months as the last n WHO-observed months inside the training window.\"\"\"\n",
    "    idx_who = np.where(train_mask_full & mask_who)[0]\n",
    "    if len(idx_who) <= n_val_who_months:\n",
    "        # Not enough WHO months to form a validation set\n",
    "        return train_mask_full.copy(), np.zeros_like(train_mask_full, dtype=bool)\n",
    "    val_idx = idx_who[-n_val_who_months:]\n",
    "    val_mask = np.zeros_like(train_mask_full, dtype=bool)\n",
    "    val_mask[val_idx] = True\n",
    "    train_inner = train_mask_full & (~val_mask)\n",
    "    return train_inner, val_mask\n",
    "\n",
    "\n",
    "tuning_splits = list(rolling_split_months[: int(getattr(cfg, 'tune_n_splits', 3))])\n",
    "lambda_year_grid = list(getattr(cfg, 'tune_lambda_year_grid', (0.0, 0.1, 0.3, 1.0, 3.0)))\n",
    "lambda_reg_grid = list(getattr(cfg, 'tune_lambda_reg_grid', (1e-6, 1e-5, 1e-4, 1e-3, 1e-2)))\n",
    "inner_val_n = int(getattr(cfg, 'inner_val_n_who_months', 3))\n",
    "\n",
    "tune_rows = []\n",
    "print(\"Tuning on early splits:\", tuning_splits)\n",
    "for ly in lambda_year_grid:\n",
    "    for lr in lambda_reg_grid:\n",
    "        val_rmses = []\n",
    "        val_mapes = []\n",
    "        year_mapes = []\n",
    "        used = 0\n",
    "\n",
    "        for split_month_tune in tuning_splits:\n",
    "            train_mask_full, _ = time_series_split_mask(X_df.index, split_month_tune)\n",
    "            train_inner_mask, val_mask = make_inner_train_val_masks(train_mask_full, mask_who, inner_val_n)\n",
    "\n",
    "            # If we cannot form a validation set, skip this split\n",
    "            if not val_mask.any():\n",
    "                continue\n",
    "\n",
    "            # Standardize using TRAIN-INNER only (avoid leakage into validation months)\n",
    "            if cfg.standardize_features:\n",
    "                X_tune, feat_mean_t, feat_std_t = standardize_features(X_raw, train_inner_mask, skip_cols=skip_cols)\n",
    "            else:\n",
    "                X_tune = X_raw.copy()\n",
    "                feat_mean_t, feat_std_t = None, None\n",
    "\n",
    "            # Year constraints must also be built from TRAIN-INNER only (avoid leaking val months through L_year)\n",
    "            year_constraints_t = build_year_constraints(X_df.index, yearly_proxy, train_inner_mask)\n",
    "\n",
    "            cfg_t = copy.deepcopy(cfg)\n",
    "            cfg_t.split_month = split_month_tune\n",
    "            cfg_t.lambda_year = float(ly)\n",
    "            cfg_t.lambda_reg = float(lr)\n",
    "            cfg_t.epochs = int(getattr(cfg, 'tune_epochs', 6000))\n",
    "            cfg_t.patience = int(getattr(cfg, 'tune_patience', 300))\n",
    "\n",
    "            beta_hat_t, _loss_df_t = train_step2_joint_loss(\n",
    "                X=X_tune,\n",
    "                y_who=y_who,\n",
    "                mask_who=mask_who,\n",
    "                year_constraints=year_constraints_t,\n",
    "                train_mask=train_inner_mask,\n",
    "                cfg=cfg_t,\n",
    "                feature_cols=feature_cols,\n",
    "            )\n",
    "\n",
    "            y_pred_t = make_predictions(X_tune, np.asarray(beta_hat_t, dtype=float), cfg_t)\n",
    "\n",
    "            # Validation metrics (WHO months only)\n",
    "            val_who_mask = val_mask & mask_who\n",
    "            val_rmses.append(compute_rmse(y_who[val_who_mask], y_pred_t[val_who_mask]))\n",
    "            val_mapes.append(safe_mape(y_who[val_who_mask], y_pred_t[val_who_mask]))\n",
    "\n",
    "            # Yearly consistency on the YEARS CONSTRAINED during tuning (train-inner full years)\n",
    "            year_mapes.append(compute_yearly_mape_from_constraints(y_pred_t, year_constraints_t))\n",
    "\n",
    "            used += 1\n",
    "\n",
    "        if used == 0:\n",
    "            continue\n",
    "\n",
    "        tune_rows.append({\n",
    "            \"lambda_year\": float(ly),\n",
    "            \"lambda_reg\": float(lr),\n",
    "            \"n_splits_used\": int(used),\n",
    "            \"val_RMSE_mean\": float(np.nanmean(val_rmses)),\n",
    "            \"val_RMSE_std\": float(np.nanstd(val_rmses)),\n",
    "            \"val_MAPE_%_mean\": float(np.nanmean(val_mapes)),\n",
    "            \"year_MAPE_%_mean\": float(np.nanmean(year_mapes)),\n",
    "        })\n",
    "\n",
    "tune_df = pd.DataFrame(tune_rows)\n",
    "tune_df = tune_df.sort_values([\"val_RMSE_mean\", \"year_MAPE_%_mean\"]).reset_index(drop=True)\n",
    "\n",
    "# Selection rule:\n",
    "# 1) Prefer candidates with year_MAPE_%_mean <= tolerance (constraint style).\n",
    "# 2) If none satisfy, minimize composite score = val_RMSE_mean + alpha * year_MAPE_%_mean\n",
    "year_tol = float(getattr(cfg, 'year_mape_tolerance', 0.10)) * 100.0\n",
    "alpha_year = float(getattr(cfg, 'alpha_year_score', 0.5))\n",
    "\n",
    "tune_df[\"meets_year_tol\"] = tune_df[\"year_MAPE_%_mean\"] <= year_tol\n",
    "tune_df[\"score_fallback\"] = tune_df[\"val_RMSE_mean\"] + alpha_year * tune_df[\"year_MAPE_%_mean\"]\n",
    "\n",
    "if tune_df[\"meets_year_tol\"].any():\n",
    "    best_row = tune_df[tune_df[\"meets_year_tol\"]].sort_values([\"val_RMSE_mean\"]).iloc[0]\n",
    "    selection_rule = f\"constraint: year_MAPE_mean <= {year_tol:.1f}% then min val_RMSE\"\n",
    "else:\n",
    "    best_row = tune_df.sort_values([\"score_fallback\"]).iloc[0]\n",
    "    selection_rule = f\"fallback: min(val_RMSE + {alpha_year:.2f} * year_MAPE)\"\n",
    "\n",
    "GLOBAL_LAMBDA_YEAR = float(best_row[\"lambda_year\"])\n",
    "GLOBAL_LAMBDA_REG = float(best_row[\"lambda_reg\"])\n",
    "\n",
    "print(\"\\nSelected GLOBAL lambdas:\")\n",
    "print(\"  lambda_year =\", GLOBAL_LAMBDA_YEAR)\n",
    "print(\"  lambda_reg  =\", GLOBAL_LAMBDA_REG)\n",
    "print(\"  selection_rule =\", selection_rule)\n",
    "\n",
    "# Save tuning table\n",
    "tune_df.to_csv(roll_dir / \"lambda_tuning_table.csv\", index=False)\n",
    "print(\"Saved lambda tuning table to:\", (roll_dir / \"lambda_tuning_table.csv\").resolve())\n",
    "\n",
    "for split_month in rolling_split_months:\n",
    "    train_mask, test_mask = time_series_split_mask(X_df.index, split_month)\n",
    "\n",
    "    if int(train_mask.sum()) == 0 or int(test_mask.sum()) == 0:\n",
    "        continue\n",
    "\n",
    "    # Standardize features (using training set statistics only, skip intercept)\n",
    "    if cfg.standardize_features:\n",
    "        X_proc, feat_mean, feat_std = standardize_features(X_raw, train_mask, skip_cols=skip_cols)\n",
    "    else:\n",
    "        X_proc = X_raw.copy()\n",
    "        feat_mean = None\n",
    "        feat_std = None\n",
    "\n",
    "    # Year constraints (only full years entirely in training window)\n",
    "    year_constraints = build_year_constraints(X_df.index, yearly_proxy, train_mask)\n",
    "    year_years = [int(t[0]) for t in year_constraints]\n",
    "\n",
    "    # Copy cfg for this split (optionally override training budget for rolling)\n",
    "    cfg_i = copy.deepcopy(cfg)\n",
    "    cfg_i.split_month = split_month\n",
    "    cfg_i.epochs = int(getattr(cfg, 'rolling_max_epochs', cfg.epochs))\n",
    "    cfg_i.patience = int(getattr(cfg, 'rolling_patience', cfg.patience))\n",
    "\n",
    "    # Use the globally tuned lambdas for all splits\n",
    "    cfg_i.lambda_year = float(GLOBAL_LAMBDA_YEAR)\n",
    "    cfg_i.lambda_reg  = float(GLOBAL_LAMBDA_REG)\n",
    "\n",
    "    beta_hat, loss_df = train_step2_joint_loss(\n",
    "        X=X_proc,\n",
    "        y_who=y_who,\n",
    "        mask_who=mask_who,\n",
    "        year_constraints=year_constraints,\n",
    "        train_mask=train_mask,\n",
    "        cfg=cfg_i,\n",
    "        feature_cols=feature_cols,\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Coefficients per split\n",
    "    # -------------------------\n",
    "    beta_hat = np.asarray(beta_hat, dtype=float)\n",
    "\n",
    "    # Convert to WHO units (because make_predictions multiplies by target_scale)\n",
    "    coef_y_units_per_stdX = beta_hat * float(cfg_i.target_scale)\n",
    "\n",
    "    # Convert coefficients back to *raw* feature scale so they are comparable across splits\n",
    "    # If X_j was standardized as (raw - mean)/std, then:\n",
    "    #   y = sum_j (coef/std_j) * raw_j  +  (intercept - sum_j (coef/std_j)*mean_j)\n",
    "    if cfg.standardize_features and (feat_mean is not None) and (feat_std is not None):\n",
    "        coef_y_units_per_rawX = coef_y_units_per_stdX / feat_std\n",
    "        if intercept_idx is not None:\n",
    "            intercept_raw = coef_y_units_per_stdX[intercept_idx] - float(np.dot(coef_y_units_per_rawX, feat_mean))\n",
    "            coef_y_units_per_rawX = coef_y_units_per_rawX.copy()\n",
    "            coef_y_units_per_rawX[intercept_idx] = intercept_raw\n",
    "    else:\n",
    "        coef_y_units_per_rawX = coef_y_units_per_stdX.copy()\n",
    "\n",
    "    for j, name in enumerate(feature_cols):\n",
    "        coef_rows.append({\n",
    "            'split_month': split_month,\n",
    "            'feature': name,\n",
    "            'beta_hat': float(beta_hat[j]),\n",
    "            'coef_y_units_per_stdX': float(coef_y_units_per_stdX[j]),\n",
    "            'coef_y_units_per_rawX': float(coef_y_units_per_rawX[j]),\n",
    "        })\n",
    "\n",
    "    # Predict for all months\n",
    "    x_pred = make_predictions(X_proc, beta_hat, cfg_i)\n",
    "\n",
    "    # Evaluation (only on months with WHO observations)\n",
    "    mask_train_who = train_mask & mask_who\n",
    "    mask_test_who = test_mask & mask_who\n",
    "\n",
    "    rmse_train = compute_rmse(y_who[mask_train_who], x_pred[mask_train_who]) if mask_train_who.any() else np.nan\n",
    "    mape_train = safe_mape(y_who[mask_train_who], x_pred[mask_train_who]) if mask_train_who.any() else np.nan\n",
    "    rmse_test = compute_rmse(y_who[mask_test_who], x_pred[mask_test_who]) if mask_test_who.any() else np.nan\n",
    "    mape_test = safe_mape(y_who[mask_test_who], x_pred[mask_test_who]) if mask_test_who.any() else np.nan\n",
    "\n",
    "    # Seasonal naive baseline using last year's same month (if lag{seasonal_period} feature exists)\n",
    "    rmse_test_seasonal_naive = np.nan\n",
    "    mape_test_seasonal_naive = np.nan\n",
    "    seasonal_lag_name = f'x_tilde_lag{int(getattr(cfg_i, \"seasonal_period\", 12))}'\n",
    "    if seasonal_lag_name in feature_cols:\n",
    "        j_lag = feature_cols.index(seasonal_lag_name)\n",
    "        seasonal_naive = X_raw[:, j_lag] * float(cfg_i.target_scale)  # back to WHO units\n",
    "        rmse_test_seasonal_naive = compute_rmse(y_who[mask_test_who], seasonal_naive[mask_test_who]) if mask_test_who.any() else np.nan\n",
    "        mape_test_seasonal_naive = safe_mape(y_who[mask_test_who], seasonal_naive[mask_test_who]) if mask_test_who.any() else np.nan\n",
    "\n",
    "\n",
    "    metric_rows.append({\n",
    "        'split_month': split_month,\n",
    "        'n_train_months': int(train_mask.sum()),\n",
    "        'n_test_months': int(test_mask.sum()),\n",
    "        'n_train_who': int(mask_train_who.sum()),\n",
    "        'n_test_who': int(mask_test_who.sum()),\n",
    "        'n_year_constraints': int(len(year_constraints)),\n",
    "        'year_constraint_years': ';'.join(map(str, year_years)) if len(year_years) else '',\n",
    "        'RMSE_train': rmse_train,\n",
    "        'MAPE_train_%': mape_train,\n",
    "        'RMSE_test': rmse_test,\n",
    "        'MAPE_test_%': mape_test,\n",
    "        'RMSE_test_seasonal_naive': rmse_test_seasonal_naive,\n",
    "        'MAPE_test_seasonal_naive_%': mape_test_seasonal_naive,\n",
    "        'epochs_used': int(loss_df['epoch'].max()) if (not loss_df.empty and 'epoch' in loss_df.columns) else np.nan,\n",
    "    })\n",
    "\n",
    "    # Plot: prediction vs WHO (with split line)\n",
    "    fname = f'who_vs_pred_step2_split_{split_month}.png'\n",
    "    title = f'WHO vs Predicted (Step 2)  split_month={split_month}'\n",
    "    plot_who_vs_pred(X_df.index, y_who, x_pred, train_mask, roll_dir, fname=fname, title=title)\n",
    "\n",
    "# -------------------------\n",
    "# Save metrics + coefficients\n",
    "# -------------------------\n",
    "rolling_metrics_df = pd.DataFrame(metric_rows).sort_values('split_month')\n",
    "coef_long_df = pd.DataFrame(coef_rows)\n",
    "\n",
    "rolling_metrics_df.to_csv(roll_dir / 'rolling_split_metrics.csv', index=False)\n",
    "coef_long_df.to_csv(roll_dir / 'rolling_split_coefficients_long.csv', index=False)\n",
    "\n",
    "# Wide-format coefficient tables (easier to scan in Excel)\n",
    "coef_raw_wide = coef_long_df.pivot(index='split_month', columns='feature', values='coef_y_units_per_rawX').reset_index()\n",
    "coef_std_wide = coef_long_df.pivot(index='split_month', columns='feature', values='coef_y_units_per_stdX').reset_index()\n",
    "coef_raw_wide.to_csv(roll_dir / 'rolling_split_coefficients_raw_wide.csv', index=False)\n",
    "coef_std_wide.to_csv(roll_dir / 'rolling_split_coefficients_std_wide.csv', index=False)\n",
    "\n",
    "print('Saved rolling plots to:', roll_dir.resolve())\n",
    "print('Saved metrics to:', (roll_dir / 'rolling_split_metrics.csv').resolve())\n",
    "print('Saved coefficients (long) to:', (roll_dir / 'rolling_split_coefficients_long.csv').resolve())\n",
    "print('Saved coefficients (wide/raw) to:', (roll_dir / 'rolling_split_coefficients_raw_wide.csv').resolve())\n",
    "print('Saved coefficients (wide/std) to:', (roll_dir / 'rolling_split_coefficients_std_wide.csv').resolve())\n",
    "\n",
    "rolling_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5363918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rolling plots to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits\n",
      "Saved metrics to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits/rolling_split_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Save rolling metrics table\n",
    "if 'rolling_metrics_df' in globals() and len(rolling_metrics_df) > 0:\n",
    "    rolling_metrics_df.to_csv(roll_dir / 'rolling_split_metrics.csv', index=False)\n",
    "    print('Saved rolling plots to:', roll_dir.resolve())\n",
    "    print('Saved metrics to:', (roll_dir / 'rolling_split_metrics.csv').resolve())\n",
    "else:\n",
    "    print('No rolling metrics produced. Check rolling_split_months and data coverage.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94bd02e",
   "metadata": {},
   "source": [
    "## 3) Plot and Save Outputs\n",
    "\n",
    "Includes:\n",
    "- Training loss curve\n",
    "- WHO vs predictions (with train/test split line)\n",
    "- Yearly aggregation vs OpenDengue\n",
    "- Step1 vs Step2 comparison\n",
    "- Parameter and prediction tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c506c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: keep the original single-split outputs if you still want them.\n",
    "# Set cfg.split_month to one of the rolling months (or any month) and re-run the original pipeline.\n",
    "#\n",
    "# Example:\n",
    "#   cfg.split_month = '2025-04'\n",
    "#   train_mask, test_mask = time_series_split_mask(X_df.index, cfg.split_month)\n",
    "#   ... (then follow the original cells 8-11 logic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
