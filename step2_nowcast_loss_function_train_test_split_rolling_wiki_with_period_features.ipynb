{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f629dbf",
   "metadata": {},
   "source": [
    "# Step 2 Dengue Nowcasting (ARX + Joint Loss)\n",
    "\n",
    "This notebook implements Step 2 according to the requirements:\n",
    "- Training set: months **<= split_month** (rolling splits)\n",
    "- Test set: months **> split_month** (for evaluation only)\n",
    "- Joint loss: WHO monthly supervision + OpenDengue yearly constraints + L2 regularization\n",
    "- Use Step 1 estimates to impute missing WHO observations as $\\tilde{x}$ for lagged inputs\n",
    "- Output complete parameters, training curves, predictions, and plots\n",
    "\n",
    "Overfitting control: L2 regularization + early stopping + train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae4f6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install dependencies in your environment:\n",
    "# pip install -U pandas numpy torch matplotlib\n",
    "\n",
    "# Prevent kernel crashes from OpenMP/MKL library conflicts\n",
    "import os\n",
    "os.environ.setdefault('KMP_DUPLICATE_LIB_OK', 'TRUE')\n",
    "os.environ.setdefault('OMP_NUM_THREADS', '1')\n",
    "os.environ.setdefault('MKL_NUM_THREADS', '1')\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Further limit thread count\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1261ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Step2Config(data_path='master_data.csv', step1_predictions_path='outputs_step1_wiki/predictions_step1_monthly.csv', step1_pred_col='x_pred', start_month='2021-01', rolling_start='2024-11', rolling_end='2025-08', rolling_max_epochs=8000, rolling_patience=400, target_source='WHO', google_sources=('Google_Trends_Dengue_fever', 'Google_Trends_Dengue_vaccine'), use_wiki=True, wiki_path='total_dengue_views.csv', wiki_month_col='Month', wiki_value_col='Total_Views', wiki_transform='log1p', lags_y=(1, 2, 12), use_month_dummies=False, seasonal_period=12, use_fourier=True, fourier_K=2, yearly_proxy_sources_priority=('OpenDengue_State_Aggregated', 'OpenDengue_National_Yearly'), lambda_who=5.0, lambda_year=0.5, lambda_reg=0.0001, lambda_lag_reg=0.005, epochs=30000, lr=0.003, seed=42, target_scale=1000.0, early_stop=True, patience=1000, min_delta=1e-07, standardize_features=True, outdir='outputs_step2_period', clip_nonnegative=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Step2Config:\n",
    "    # Inputs\n",
    "    data_path: str = 'master_data.csv'\n",
    "    step1_predictions_path: str = os.path.join('outputs_step1_wiki', 'predictions_step1_monthly.csv')\n",
    "    step1_pred_col: str = 'x_pred'\n",
    "\n",
    "    # Modeling window\n",
    "    start_month: str = '2021-01'\n",
    "\n",
    "    # Rolling split evaluation (inclusive)\n",
    "    rolling_start: str = '2024-11'\n",
    "    rolling_end: str = '2025-08'\n",
    "    # To keep rolling evaluation runtime reasonable, you can override training budget per split\n",
    "    rolling_max_epochs: int = 8000\n",
    "    rolling_patience: int = 400\n",
    "\n",
    "    # Target/source names in master_data.csv\n",
    "    target_source: str = 'WHO'\n",
    "\n",
    "    # Exogenous signals\n",
    "    google_sources: Tuple[str, str] = (\n",
    "        'Google_Trends_Dengue_fever',\n",
    "        'Google_Trends_Dengue_vaccine',\n",
    "    )\n",
    "\n",
    "\n",
    "    # Wikipedia pageviews (external regressor)\n",
    "    use_wiki: bool = True\n",
    "    wiki_path: str = 'total_dengue_views.csv'\n",
    "    wiki_month_col: str = 'Month'\n",
    "    wiki_value_col: str = 'Total_Views'\n",
    "    wiki_transform: str = 'log1p'  # 'log1p' or 'none'\n",
    "\n",
    "    # Feature engineering\n",
    "    lags_y: Tuple[int, ...] = (1, 2, 12)  # include seasonal lag 12 for monthly data\n",
    "    use_month_dummies: bool = False  # set False to use Fourier terms instead\n",
    "\n",
    "    # Seasonality (periodic time series)\n",
    "    seasonal_period: int = 12  # monthly seasonality\n",
    "    use_fourier: bool = True  # set True to use Fourier terms instead of month dummies\n",
    "    fourier_K: int = 2         # number of Fourier harmonics\n",
    "    # OpenDengue yearly proxy sources (priority)\n",
    "    yearly_proxy_sources_priority: Tuple[str, ...] = (\n",
    "        'OpenDengue_State_Aggregated',\n",
    "        'OpenDengue_National_Yearly',\n",
    "    )\n",
    "\n",
    "    # Loss weights - Conservative: balance all three components\n",
    "    lambda_who: float = 5.0  # Moderate WHO weight\n",
    "    lambda_year: float = 0.5  # Strong yearly constraint to anchor predictions\n",
    "    lambda_reg: float = 1e-4  # Base regularization\n",
    "    lambda_lag_reg: float = 5e-3  # Moderate lag regularization\n",
    "\n",
    "    # Optimization\n",
    "    epochs: int = 30000  \n",
    "    lr: float = 3e-3  # Slower learning rate for stability\n",
    "    seed: int = 42\n",
    "    target_scale: float = 1000.0\n",
    "\n",
    "    # Overfitting controls\n",
    "    early_stop: bool = True\n",
    "    patience: int = 1000  # Large patience for stable convergence\n",
    "    min_delta: float = 1e-7  # Smaller threshold for early stopping\n",
    "\n",
    "    # Scaling features (standardize using train stats)\n",
    "    standardize_features: bool = True\n",
    "\n",
    "    # Output\n",
    "    outdir: str = 'outputs_step2_period'\n",
    "    clip_nonnegative: bool = True\n",
    "\n",
    "\n",
    "cfg = Step2Config()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3e6b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_outdir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def parse_monthly_date_any(s: pd.Series) -> pd.Series:\n",
    "    x = s.astype(str)\n",
    "    is_ym = x.str.match('^[0-9]{4}-[0-9]{2}$')\n",
    "    x = np.where(is_ym, x + '-01', x)\n",
    "    dt = pd.to_datetime(x, errors='coerce')\n",
    "    return dt\n",
    "\n",
    "\n",
    "def load_master_csv(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    expected = {'resolution', 'date', 'value', 'source'}\n",
    "    missing = expected - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"master_data.csv missing columns: {sorted(missing)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_monthly_wide(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    m = df[df['resolution'].astype(str).str.lower().eq('monthly')].copy()\n",
    "    m['date'] = parse_monthly_date_any(m['date'])\n",
    "    m = m.dropna(subset=['date']).copy()\n",
    "    wide = (\n",
    "        m.pivot_table(index='date', columns='source', values='value', aggfunc='mean')\n",
    "        .sort_index()\n",
    "    )\n",
    "    return wide\n",
    "\n",
    "\n",
    "def build_yearly_proxy(df: pd.DataFrame, priority_sources: Tuple[str, ...]) -> pd.DataFrame:\n",
    "    y = df[df['resolution'].astype(str).str.lower().eq('yearly')].copy()\n",
    "    y['year'] = pd.to_numeric(y['date'], errors='coerce').astype('Int64')\n",
    "    y = y.dropna(subset=['year'])\n",
    "    y['year'] = y['year'].astype(int)\n",
    "\n",
    "    pivot = (\n",
    "        y.pivot_table(index='year', columns='source', values='value', aggfunc='mean')\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    chosen = []\n",
    "    for year, row in pivot.iterrows():\n",
    "        val = np.nan\n",
    "        src = None\n",
    "        for s in priority_sources:\n",
    "            if s in row.index and pd.notna(row[s]):\n",
    "                val = float(row[s])\n",
    "                src = s\n",
    "                break\n",
    "        if pd.notna(val):\n",
    "            chosen.append((year, val, src))\n",
    "\n",
    "    return pd.DataFrame(chosen, columns=['year', 'od_total', 'od_source'])\n",
    "\n",
    "\n",
    "def load_step1_predictions(cfg: Step2Config) -> pd.Series:\n",
    "    if not os.path.exists(cfg.step1_predictions_path):\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    s1 = pd.read_csv(cfg.step1_predictions_path)\n",
    "    if 'date' not in s1.columns:\n",
    "        raise ValueError(f\"Step 1 predictions file missing 'date': {cfg.step1_predictions_path}\")\n",
    "\n",
    "    s1['date'] = parse_monthly_date_any(s1['date'])\n",
    "    s1 = s1.dropna(subset=['date']).copy()\n",
    "    s1 = s1.sort_values('date')\n",
    "\n",
    "    col = cfg.step1_pred_col if cfg.step1_pred_col in s1.columns else None\n",
    "    if col is None:\n",
    "        cand = [c for c in s1.columns if 'pred' in c.lower()]\n",
    "        if not cand:\n",
    "            cand = [c for c in s1.columns if c != 'date']\n",
    "        if not cand:\n",
    "            raise ValueError('No prediction column found in Step 1 predictions file.')\n",
    "        col = cand[0]\n",
    "\n",
    "    ser = s1.set_index('date')[col].astype(float)\n",
    "    return ser\n",
    "\n",
    "\n",
    "\n",
    "def load_wiki_monthly(cfg: Step2Config) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Load monthly Wikipedia pageviews time series.\n",
    "    Expected columns: cfg.wiki_month_col (YYYY-MM) and cfg.wiki_value_col (numeric)\n",
    "    Returns: pd.Series indexed by month (Timestamp at month start).\n",
    "    \"\"\"\n",
    "    if (not getattr(cfg, 'use_wiki', False)) or (not os.path.exists(cfg.wiki_path)):\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    w = pd.read_csv(cfg.wiki_path)\n",
    "    if cfg.wiki_month_col not in w.columns or cfg.wiki_value_col not in w.columns:\n",
    "        raise ValueError(\n",
    "            f\"Wiki file must contain columns '{cfg.wiki_month_col}' and '{cfg.wiki_value_col}'. Got: {list(w.columns)}\"\n",
    "        )\n",
    "\n",
    "    w = w.copy()\n",
    "    w['date'] = parse_monthly_date_any(w[cfg.wiki_month_col])\n",
    "    w = w.dropna(subset=['date']).sort_values('date')\n",
    "    s = pd.to_numeric(w[cfg.wiki_value_col], errors='coerce')\n",
    "    ser = pd.Series(s.values, index=w['date']).groupby(level=0).mean()\n",
    "    return ser\n",
    "\n",
    "def add_month_dummies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dt = pd.to_datetime(df.index)\n",
    "    m = pd.get_dummies(dt.month, prefix='m', drop_first=True)\n",
    "    m.index = df.index\n",
    "    return pd.concat([df, m.astype(float)], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def add_fourier_features(df: pd.DataFrame, period: int = 12, K: int = 2, prefix: str = 's') -> pd.DataFrame:\n",
    "    \"\"\"Add Fourier seasonality terms based on month-of-year.\"\"\"\n",
    "    dt = pd.to_datetime(df.index)\n",
    "    m = dt.month.astype(float)\n",
    "    out = df.copy()\n",
    "    for k in range(1, int(K) + 1):\n",
    "        out[f'{prefix}_sin{k}'] = np.sin(2 * np.pi * k * m / float(period))\n",
    "        out[f'{prefix}_cos{k}'] = np.cos(2 * np.pi * k * m / float(period))\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def build_design_matrix(wide: pd.DataFrame, cfg: Step2Config) -> tuple[pd.DataFrame, np.ndarray, np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Build design matrix.\n",
    "    Critical fix: lagged variables x_tilde must be scaled by target_scale to match target scale.\n",
    "    \"\"\"\n",
    "    start_dt = pd.to_datetime(cfg.start_month + '-01')\n",
    "    wide = wide.loc[wide.index >= start_dt].copy()\n",
    "\n",
    "    target = cfg.target_source\n",
    "    if target not in wide.columns:\n",
    "        wide[target] = np.nan\n",
    "\n",
    "    g1, g2 = cfg.google_sources\n",
    "    for g in [g1, g2]:\n",
    "        if g not in wide.columns:\n",
    "            raise ValueError(f\"Missing Google source '{g}' in monthly data.\")\n",
    "\n",
    "    y_who = pd.to_numeric(wide[target], errors='coerce')\n",
    "\n",
    "    step1_ser = load_step1_predictions(cfg)\n",
    "    wide['STEP1_est'] = step1_ser.reindex(wide.index)\n",
    "\n",
    "    # x_tilde: WHO when observed, else Step1 estimate (for lagged inputs)\n",
    "    x_tilde = y_who.where(y_who.notna(), wide['STEP1_est'])\n",
    "    \n",
    "    # Critical fix: scale lagged variables to match target scale\n",
    "    x_tilde_scaled = x_tilde / cfg.target_scale\n",
    "\n",
    "    X_df = pd.DataFrame(index=wide.index)\n",
    "    \n",
    "    # Normalize Google Trends from [0, 100] to [0, 1]\n",
    "    X_df['g_fever'] = pd.to_numeric(wide[g1], errors='coerce') / 100.0\n",
    "    X_df['g_vaccine'] = pd.to_numeric(wide[g2], errors='coerce') / 100.0\n",
    "\n",
    "\n",
    "    # Wikipedia pageviews regressor\n",
    "    if getattr(cfg, 'use_wiki', False):\n",
    "        wiki_ser = load_wiki_monthly(cfg)\n",
    "        wide['WIKI_raw'] = wiki_ser.reindex(wide.index)\n",
    "        wiki_raw = pd.to_numeric(wide['WIKI_raw'], errors='coerce')\n",
    "        wiki_raw = wiki_raw.interpolate(limit_direction='both').ffill().bfill()\n",
    "        if str(getattr(cfg, 'wiki_transform', 'log1p')).lower() == 'log1p':\n",
    "            X_df['wiki_views'] = np.log1p(wiki_raw.astype(float))\n",
    "        else:\n",
    "            X_df['wiki_views'] = wiki_raw.astype(float)\n",
    "\n",
    "    # Use scaled values for lagged variables\n",
    "    for k in cfg.lags_y:\n",
    "        X_df[f'x_tilde_lag{k}'] = x_tilde_scaled.shift(k)\n",
    "\n",
    "    # Seasonality features (periodic time series)\n",
    "    # Prefer Fourier terms when enabled; otherwise fall back to month dummies.\n",
    "    if getattr(cfg, 'use_fourier', False):\n",
    "        X_df = add_fourier_features(\n",
    "            X_df,\n",
    "            period=int(getattr(cfg, 'seasonal_period', 12)),\n",
    "            K=int(getattr(cfg, 'fourier_K', 2)),\n",
    "            prefix='s'\n",
    "        )\n",
    "    elif cfg.use_month_dummies:\n",
    "        X_df = add_month_dummies(X_df)\n",
    "\n",
    "\n",
    "    X_df['intercept'] = 1.0\n",
    "\n",
    "    # Light imputation on exogenous features only (lags can remain NaN and be imputed later)\n",
    "    exo_cols = ['g_fever', 'g_vaccine'] + (['wiki_views'] if 'wiki_views' in X_df.columns else [])\n",
    "    for col in exo_cols:\n",
    "        X_df[col] = X_df[col].interpolate(limit_direction='both').ffill().bfill()\n",
    "\n",
    "    feature_cols = [c for c in X_df.columns if c != 'intercept'] + ['intercept']\n",
    "    X = X_df[feature_cols].to_numpy(dtype=np.float32)\n",
    "    y = y_who.to_numpy(dtype=np.float32)\n",
    "    mask_who = np.isfinite(y)\n",
    "\n",
    "    return X_df, X, y, mask_who, feature_cols\n",
    "\n",
    "\n",
    "def build_year_constraints(dates: pd.DatetimeIndex, yearly_proxy: pd.DataFrame, train_mask: np.ndarray) -> List[tuple]:\n",
    "    df_dates = pd.DataFrame({'date': pd.to_datetime(dates)})\n",
    "    df_dates['year'] = df_dates['date'].dt.year\n",
    "    df_dates['month'] = df_dates['date'].dt.month\n",
    "\n",
    "    constraints = []\n",
    "    for _, r in yearly_proxy.iterrows():\n",
    "        y = int(r['year'])\n",
    "        od_total = float(r['od_total'])\n",
    "        od_src = str(r['od_source'])\n",
    "\n",
    "        idx = df_dates.index[df_dates['year'].eq(y)].to_numpy()\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "\n",
    "        # require all 12 months and all months in training window\n",
    "        months_present = set(df_dates.loc[idx, 'month'].tolist())\n",
    "        if months_present != set(range(1, 13)):\n",
    "            continue\n",
    "        if not np.all(train_mask[idx]):\n",
    "            continue\n",
    "\n",
    "        constraints.append((y, idx, od_total, od_src))\n",
    "\n",
    "    return constraints\n",
    "\n",
    "\n",
    "def time_series_split_mask(index: pd.DatetimeIndex, split_month: str):\n",
    "    split_dt = pd.to_datetime(split_month + '-01')\n",
    "    train_mask = index <= split_dt\n",
    "    test_mask = index > split_dt\n",
    "    return train_mask, test_mask\n",
    "\n",
    "\n",
    "def standardize_features(X: np.ndarray, train_mask: np.ndarray, skip_cols: List[int] = None):\n",
    "    \"\"\"\n",
    "    Standardize feature matrix.\n",
    "    skip_cols: column indices to skip standardization (e.g., intercept term).\n",
    "    \"\"\"\n",
    "    X2 = X.copy()\n",
    "    n, p = X2.shape\n",
    "    means = np.zeros(p, dtype=np.float32)\n",
    "    stds = np.ones(p, dtype=np.float32)\n",
    "    \n",
    "    if skip_cols is None:\n",
    "        skip_cols = []\n",
    "\n",
    "    for j in range(p):\n",
    "        col = X2[:, j]\n",
    "        col_train = col[train_mask]\n",
    "        \n",
    "        # For intercept or specified columns, skip standardization\n",
    "        if j in skip_cols:\n",
    "            # Only impute NaN\n",
    "            col = np.where(np.isfinite(col), col, 0.0)\n",
    "            X2[:, j] = col\n",
    "            continue\n",
    "        \n",
    "        m = np.nanmean(col_train)\n",
    "        s = np.nanstd(col_train)\n",
    "        if not np.isfinite(m):\n",
    "            m = 0.0\n",
    "        if (not np.isfinite(s)) or s <= 1e-12:\n",
    "            s = 1.0\n",
    "        means[j] = m\n",
    "        stds[j] = s\n",
    "\n",
    "        # impute NaNs with train mean, then standardize\n",
    "        col = np.where(np.isfinite(col), col, m)\n",
    "        X2[:, j] = (col - m) / s\n",
    "\n",
    "    return X2, means, stds\n",
    "\n",
    "\n",
    "def train_step2_joint_loss(\n",
    "    X: np.ndarray,\n",
    "    y_who: np.ndarray,\n",
    "    mask_who: np.ndarray,\n",
    "    year_constraints: List[tuple],\n",
    "    train_mask: np.ndarray,\n",
    "    cfg: Step2Config,\n",
    "    feature_cols: List[str],\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train with differentiated regularization: stronger penalty on lag coefficients.\n",
    "    \"\"\"\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    y_scaled = y_who / cfg.target_scale\n",
    "    y_t = torch.tensor(y_scaled, dtype=torch.float32, device=device)\n",
    "    mask_who_t = torch.tensor(mask_who & train_mask, dtype=torch.bool, device=device)\n",
    "\n",
    "    year_terms = []\n",
    "    for (year, idx, od_total, od_src) in year_constraints:\n",
    "        year_terms.append(\n",
    "            (year, torch.tensor(idx, dtype=torch.long, device=device), float(od_total / cfg.target_scale), od_src)\n",
    "        )\n",
    "\n",
    "    # Identify lag feature indices for differentiated regularization\n",
    "    lag_indices = [i for i, col in enumerate(feature_cols) if 'lag' in col.lower()]\n",
    "\n",
    "    p = X_t.shape[1]\n",
    "    beta = torch.nn.Parameter(torch.zeros(p, dtype=torch.float32, device=device))\n",
    "    opt = torch.optim.Adam([beta], lr=cfg.lr)\n",
    "\n",
    "    rows = []\n",
    "    best_loss = np.inf\n",
    "    best_beta = None\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        x = X_t @ beta\n",
    "\n",
    "        if mask_who_t.any():\n",
    "            diff_who = x[mask_who_t] - y_t[mask_who_t]\n",
    "            L_who = (diff_who ** 2).mean()\n",
    "        else:\n",
    "            L_who = torch.tensor(0.0, device=device)\n",
    "\n",
    "        if len(year_terms) > 0:\n",
    "            diffs = []\n",
    "            for _, idx_t, od_total_scaled, _ in year_terms:\n",
    "                year_sum = x.index_select(0, idx_t).sum()\n",
    "                diffs.append((year_sum - od_total_scaled) ** 2)\n",
    "            L_year = torch.stack(diffs).mean()\n",
    "        else:\n",
    "            L_year = torch.tensor(0.0, device=device)\n",
    "\n",
    "        # Differentiated regularization: base + extra penalty on lag coefficients\n",
    "        L_reg_base = (beta ** 2).sum()\n",
    "        if lag_indices:\n",
    "            beta_lag = beta[lag_indices]\n",
    "            L_reg_lag = (beta_lag ** 2).sum()\n",
    "        else:\n",
    "            L_reg_lag = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        L_reg = L_reg_base\n",
    "        L_total = cfg.lambda_who * L_who + cfg.lambda_year * L_year + cfg.lambda_reg * L_reg + cfg.lambda_lag_reg * L_reg_lag\n",
    "        L_total.backward()\n",
    "        opt.step()\n",
    "\n",
    "        val = float(L_total.detach().cpu().item())\n",
    "        rows.append({\n",
    "            'epoch': epoch,\n",
    "            'L_total': val,\n",
    "            'L_who': float(L_who.detach().cpu().item()),\n",
    "            'L_year': float(L_year.detach().cpu().item()),\n",
    "            'L_reg': float(L_reg.detach().cpu().item()),\n",
    "            'L_lag_reg': float(L_reg_lag.detach().cpu().item()) if lag_indices else 0.0,\n",
    "        })\n",
    "\n",
    "        if not np.isfinite(val):\n",
    "            raise RuntimeError('Training diverged (loss is NaN/Inf).')\n",
    "\n",
    "        if cfg.early_stop:\n",
    "            if val < best_loss - cfg.min_delta:\n",
    "                best_loss = val\n",
    "                best_beta = beta.detach().cpu().numpy().copy()\n",
    "                bad_epochs = 0\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "                if bad_epochs >= cfg.patience:\n",
    "                    break\n",
    "\n",
    "    beta_hat = beta.detach().cpu().numpy() if best_beta is None else best_beta\n",
    "    loss_df = pd.DataFrame(rows)\n",
    "    return beta_hat, loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d569c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.sqrt(np.nanmean((y_true - y_pred) ** 2)))\n",
    "\n",
    "\n",
    "def safe_mape(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = np.where(np.abs(y_true) < 1e-12, np.nan, np.abs(y_true))\n",
    "    return float(np.nanmean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "\n",
    "\n",
    "def make_predictions(X: np.ndarray, beta_hat: np.ndarray, cfg: Step2Config) -> np.ndarray:\n",
    "    x_scaled = X @ beta_hat\n",
    "    x = x_scaled * cfg.target_scale\n",
    "    if cfg.clip_nonnegative:\n",
    "        x = np.maximum(x, 0.0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def plot_loss_curve(loss_df: pd.DataFrame, outdir: Path) -> None:\n",
    "    fig = plt.figure()\n",
    "    plt.plot(loss_df['epoch'], loss_df['L_total'], label='Total')\n",
    "    if (loss_df['L_who'] != 0).any():\n",
    "        plt.plot(loss_df['epoch'], loss_df['L_who'], label='WHO')\n",
    "    if (loss_df['L_year'] != 0).any():\n",
    "        plt.plot(loss_df['epoch'], loss_df['L_year'], label='Yearly')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (log scale)')\n",
    "    plt.title('Training Loss Curve (Step 2)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outdir / 'loss_curve_step2.png', dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_who_vs_pred(dates: pd.DatetimeIndex, y_who: np.ndarray, x_pred: np.ndarray, train_mask: np.ndarray, outdir: Path, fname: str = 'who_vs_pred_step2.png', title: str = None) -> None:\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    plt.plot(dates, x_pred, label='Predicted (Step 2)')\n",
    "    plt.plot(dates, y_who, label='WHO observed')\n",
    "    split_dt = dates[train_mask].max() if train_mask.any() else None\n",
    "    if split_dt is not None:\n",
    "        plt.axvline(split_dt, linestyle='--', color='gray', label='Train/Test split')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Monthly dengue cases')\n",
    "    plt.title(title if title is not None else 'WHO vs Prediction (Step 2)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outdir / fname, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_yearly_vs_od(pred_df: pd.DataFrame, yearly_proxy: pd.DataFrame, outdir: Path) -> None:\n",
    "    pred_year = pred_df.groupby('year', as_index=False)['x_pred'].sum().rename(columns={'x_pred': 'pred_year_total'})\n",
    "    if yearly_proxy.empty:\n",
    "        return\n",
    "    merged = pred_year.merge(yearly_proxy, on='year', how='inner').sort_values('year')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(merged['year'], merged['pred_year_total'], marker='o', label='Predicted yearly sum')\n",
    "    plt.plot(merged['year'], merged['od_total'], marker='o', label='OpenDengue yearly total')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Total dengue cases (year)')\n",
    "    plt.title('Yearly Aggregation: Prediction vs OpenDengue (Step 2)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outdir / 'yearly_vs_opendengue_step2.png', dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    merged.to_csv(outdir / 'yearly_comparison_step2.csv', index=False)\n",
    "\n",
    "\n",
    "def plot_step1_vs_step2(pred_df: pd.DataFrame, outdir: Path):\n",
    "    if 'x_step1' not in pred_df.columns:\n",
    "        return\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    dates = pd.to_datetime(pred_df['date'] + '-01')\n",
    "    plt.plot(dates, pred_df['x_step1'], label='Step 1')\n",
    "    plt.plot(dates, pred_df['x_pred'], label='Step 2')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Monthly dengue cases')\n",
    "    plt.title('Step 1 vs Step 2 Predictions')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outdir / 'step1_vs_step2.png', dpi=200)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84fae24",
   "metadata": {},
   "source": [
    "## 1) Load Data, Build Features, and Split Train/Test\n",
    "\n",
    "- Use Step 1 predictions to impute missing WHO observations for $\\tilde{x}$\n",
    "- Build ARX features: Google Trends + lagged variables + month dummies\n",
    "- Training/Test split is evaluated over a rolling set of split months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d799a3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total months: 60\n",
      "WHO observed months: 23\n",
      "Feature count: 11\n",
      "Rolling split months: ['2024-11', '2024-12', '2025-01', '2025-02', '2025-03', '2025-04', '2025-05', '2025-06', '2025-07', '2025-08']\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "df = load_master_csv(cfg.data_path)\n",
    "wide = build_monthly_wide(df)\n",
    "yearly_proxy = build_yearly_proxy(df, cfg.yearly_proxy_sources_priority)\n",
    "\n",
    "X_df, X_raw, y_who, mask_who, feature_cols = build_design_matrix(wide, cfg)\n",
    "\n",
    "rolling_split_months = pd.period_range(cfg.rolling_start, cfg.rolling_end, freq='M').strftime('%Y-%m').tolist()\n",
    "\n",
    "print('Total months:', len(X_df))\n",
    "print('WHO observed months:', int(mask_who.sum()))\n",
    "print('Feature count:', X_raw.shape[1])\n",
    "print('Rolling split months:', rolling_split_months)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696bc81",
   "metadata": {},
   "source": [
    "## 2) Train Step 2 (Joint Loss + Yearly Constraints + Early Stopping)\n",
    "\n",
    "- Only training period participates in optimization\n",
    "- WHO monthly + yearly proxy + L2 regularization\n",
    "- Early stopping to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "650a8a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rolling plots to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits\n",
      "Saved metrics to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits/rolling_split_metrics.csv\n",
      "Saved coefficients (long) to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits/rolling_split_coefficients_long.csv\n",
      "Saved coefficients (wide/raw) to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits/rolling_split_coefficients_raw_wide.csv\n",
      "Saved coefficients (wide/std) to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits/rolling_split_coefficients_std_wide.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_month</th>\n",
       "      <th>n_train_months</th>\n",
       "      <th>n_test_months</th>\n",
       "      <th>n_train_who</th>\n",
       "      <th>n_test_who</th>\n",
       "      <th>n_year_constraints</th>\n",
       "      <th>year_constraint_years</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>MAPE_train_%</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>MAPE_test_%</th>\n",
       "      <th>RMSE_test_seasonal_naive</th>\n",
       "      <th>MAPE_test_seasonal_naive_%</th>\n",
       "      <th>epochs_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11</td>\n",
       "      <td>47</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2021;2022;2023</td>\n",
       "      <td>5798.417185</td>\n",
       "      <td>56.722620</td>\n",
       "      <td>9004.680134</td>\n",
       "      <td>78.251963</td>\n",
       "      <td>13004.736797</td>\n",
       "      <td>63.822491</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3352.272374</td>\n",
       "      <td>22.152340</td>\n",
       "      <td>4193.599153</td>\n",
       "      <td>47.197417</td>\n",
       "      <td>13582.735466</td>\n",
       "      <td>69.476023</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3445.289073</td>\n",
       "      <td>26.775218</td>\n",
       "      <td>5122.378979</td>\n",
       "      <td>53.832757</td>\n",
       "      <td>14245.188974</td>\n",
       "      <td>75.617242</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-02</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3435.343522</td>\n",
       "      <td>31.550900</td>\n",
       "      <td>5598.812026</td>\n",
       "      <td>51.460368</td>\n",
       "      <td>15005.998652</td>\n",
       "      <td>79.281025</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3405.577724</td>\n",
       "      <td>35.243392</td>\n",
       "      <td>6054.602613</td>\n",
       "      <td>47.290468</td>\n",
       "      <td>15899.267121</td>\n",
       "      <td>81.798489</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-04</td>\n",
       "      <td>52</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3333.246561</td>\n",
       "      <td>39.022926</td>\n",
       "      <td>6457.446141</td>\n",
       "      <td>39.459322</td>\n",
       "      <td>16995.299333</td>\n",
       "      <td>90.087162</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-05</td>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3298.995423</td>\n",
       "      <td>39.947023</td>\n",
       "      <td>6908.970094</td>\n",
       "      <td>35.308843</td>\n",
       "      <td>18348.386646</td>\n",
       "      <td>98.998839</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-06</td>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3281.308370</td>\n",
       "      <td>41.086907</td>\n",
       "      <td>6835.082545</td>\n",
       "      <td>24.163525</td>\n",
       "      <td>20094.268611</td>\n",
       "      <td>115.578294</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-07</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3255.302692</td>\n",
       "      <td>40.025636</td>\n",
       "      <td>6821.689446</td>\n",
       "      <td>22.154192</td>\n",
       "      <td>20718.814071</td>\n",
       "      <td>102.313578</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-08</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2021;2022;2023;2024</td>\n",
       "      <td>3224.948687</td>\n",
       "      <td>39.311957</td>\n",
       "      <td>8356.052437</td>\n",
       "      <td>33.330488</td>\n",
       "      <td>21525.254942</td>\n",
       "      <td>94.270769</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  split_month  n_train_months  n_test_months  n_train_who  n_test_who  \\\n",
       "0     2024-11              47             13           11          12   \n",
       "1     2024-12              48             12           12          11   \n",
       "2     2025-01              49             11           13          10   \n",
       "3     2025-02              50             10           14           9   \n",
       "4     2025-03              51              9           15           8   \n",
       "5     2025-04              52              8           16           7   \n",
       "6     2025-05              53              7           17           6   \n",
       "7     2025-06              54              6           18           5   \n",
       "8     2025-07              55              5           19           4   \n",
       "9     2025-08              56              4           20           3   \n",
       "\n",
       "   n_year_constraints year_constraint_years   RMSE_train  MAPE_train_%  \\\n",
       "0                   3        2021;2022;2023  5798.417185     56.722620   \n",
       "1                   4   2021;2022;2023;2024  3352.272374     22.152340   \n",
       "2                   4   2021;2022;2023;2024  3445.289073     26.775218   \n",
       "3                   4   2021;2022;2023;2024  3435.343522     31.550900   \n",
       "4                   4   2021;2022;2023;2024  3405.577724     35.243392   \n",
       "5                   4   2021;2022;2023;2024  3333.246561     39.022926   \n",
       "6                   4   2021;2022;2023;2024  3298.995423     39.947023   \n",
       "7                   4   2021;2022;2023;2024  3281.308370     41.086907   \n",
       "8                   4   2021;2022;2023;2024  3255.302692     40.025636   \n",
       "9                   4   2021;2022;2023;2024  3224.948687     39.311957   \n",
       "\n",
       "     RMSE_test  MAPE_test_%  RMSE_test_seasonal_naive  \\\n",
       "0  9004.680134    78.251963              13004.736797   \n",
       "1  4193.599153    47.197417              13582.735466   \n",
       "2  5122.378979    53.832757              14245.188974   \n",
       "3  5598.812026    51.460368              15005.998652   \n",
       "4  6054.602613    47.290468              15899.267121   \n",
       "5  6457.446141    39.459322              16995.299333   \n",
       "6  6908.970094    35.308843              18348.386646   \n",
       "7  6835.082545    24.163525              20094.268611   \n",
       "8  6821.689446    22.154192              20718.814071   \n",
       "9  8356.052437    33.330488              21525.254942   \n",
       "\n",
       "   MAPE_test_seasonal_naive_%  epochs_used  \n",
       "0                   63.822491         8000  \n",
       "1                   69.476023         8000  \n",
       "2                   75.617242         8000  \n",
       "3                   79.281025         8000  \n",
       "4                   81.798489         8000  \n",
       "5                   90.087162         8000  \n",
       "6                   98.998839         8000  \n",
       "7                  115.578294         8000  \n",
       "8                  102.313578         8000  \n",
       "9                   94.270769         8000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rolling train/test split evaluation: for each split_month in [rolling_start, rolling_end]\n",
    "outdir = Path(cfg.outdir)\n",
    "roll_dir = outdir / 'rolling_splits'\n",
    "ensure_outdir(str(roll_dir))\n",
    "\n",
    "metric_rows = []\n",
    "coef_rows = []\n",
    "\n",
    "# Find intercept column index (should not be standardized)\n",
    "intercept_idx = feature_cols.index('intercept') if 'intercept' in feature_cols else None\n",
    "skip_cols = [intercept_idx] if intercept_idx is not None else []\n",
    "\n",
    "for split_month in rolling_split_months:\n",
    "    train_mask, test_mask = time_series_split_mask(X_df.index, split_month)\n",
    "\n",
    "    if int(train_mask.sum()) == 0 or int(test_mask.sum()) == 0:\n",
    "        continue\n",
    "\n",
    "    # Standardize features (using training set statistics only, skip intercept)\n",
    "    if cfg.standardize_features:\n",
    "        X_proc, feat_mean, feat_std = standardize_features(X_raw, train_mask, skip_cols=skip_cols)\n",
    "    else:\n",
    "        X_proc = X_raw.copy()\n",
    "        feat_mean = None\n",
    "        feat_std = None\n",
    "\n",
    "    # Year constraints (only full years entirely in training window)\n",
    "    year_constraints = build_year_constraints(X_df.index, yearly_proxy, train_mask)\n",
    "    year_years = [int(t[0]) for t in year_constraints]\n",
    "\n",
    "    # Copy cfg for this split (optionally override training budget for rolling)\n",
    "    cfg_i = copy.deepcopy(cfg)\n",
    "    cfg_i.split_month = split_month\n",
    "    cfg_i.epochs = int(getattr(cfg, 'rolling_max_epochs', cfg.epochs))\n",
    "    cfg_i.patience = int(getattr(cfg, 'rolling_patience', cfg.patience))\n",
    "\n",
    "    beta_hat, loss_df = train_step2_joint_loss(\n",
    "        X=X_proc,\n",
    "        y_who=y_who,\n",
    "        mask_who=mask_who,\n",
    "        year_constraints=year_constraints,\n",
    "        train_mask=train_mask,\n",
    "        cfg=cfg_i,\n",
    "        feature_cols=feature_cols,\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Coefficients per split\n",
    "    # -------------------------\n",
    "    beta_hat = np.asarray(beta_hat, dtype=float)\n",
    "\n",
    "    # Convert to WHO units (because make_predictions multiplies by target_scale)\n",
    "    coef_y_units_per_stdX = beta_hat * float(cfg_i.target_scale)\n",
    "\n",
    "    # Convert coefficients back to *raw* feature scale so they are comparable across splits\n",
    "    # If X_j was standardized as (raw - mean)/std, then:\n",
    "    #   y = sum_j (coef/std_j) * raw_j  +  (intercept - sum_j (coef/std_j)*mean_j)\n",
    "    if cfg.standardize_features and (feat_mean is not None) and (feat_std is not None):\n",
    "        coef_y_units_per_rawX = coef_y_units_per_stdX / feat_std\n",
    "        if intercept_idx is not None:\n",
    "            intercept_raw = coef_y_units_per_stdX[intercept_idx] - float(np.dot(coef_y_units_per_rawX, feat_mean))\n",
    "            coef_y_units_per_rawX = coef_y_units_per_rawX.copy()\n",
    "            coef_y_units_per_rawX[intercept_idx] = intercept_raw\n",
    "    else:\n",
    "        coef_y_units_per_rawX = coef_y_units_per_stdX.copy()\n",
    "\n",
    "    for j, name in enumerate(feature_cols):\n",
    "        coef_rows.append({\n",
    "            'split_month': split_month,\n",
    "            'feature': name,\n",
    "            'beta_hat': float(beta_hat[j]),\n",
    "            'coef_y_units_per_stdX': float(coef_y_units_per_stdX[j]),\n",
    "            'coef_y_units_per_rawX': float(coef_y_units_per_rawX[j]),\n",
    "        })\n",
    "\n",
    "    # Predict for all months\n",
    "    x_pred = make_predictions(X_proc, beta_hat, cfg_i)\n",
    "\n",
    "    # Evaluation (only on months with WHO observations)\n",
    "    mask_train_who = train_mask & mask_who\n",
    "    mask_test_who = test_mask & mask_who\n",
    "\n",
    "    rmse_train = compute_rmse(y_who[mask_train_who], x_pred[mask_train_who]) if mask_train_who.any() else np.nan\n",
    "    mape_train = safe_mape(y_who[mask_train_who], x_pred[mask_train_who]) if mask_train_who.any() else np.nan\n",
    "    rmse_test = compute_rmse(y_who[mask_test_who], x_pred[mask_test_who]) if mask_test_who.any() else np.nan\n",
    "    mape_test = safe_mape(y_who[mask_test_who], x_pred[mask_test_who]) if mask_test_who.any() else np.nan\n",
    "\n",
    "    # Seasonal naive baseline using last year's same month (if lag{seasonal_period} feature exists)\n",
    "    rmse_test_seasonal_naive = np.nan\n",
    "    mape_test_seasonal_naive = np.nan\n",
    "    seasonal_lag_name = f'x_tilde_lag{int(getattr(cfg_i, \"seasonal_period\", 12))}'\n",
    "    if seasonal_lag_name in feature_cols:\n",
    "        j_lag = feature_cols.index(seasonal_lag_name)\n",
    "        seasonal_naive = X_raw[:, j_lag] * float(cfg_i.target_scale)  # back to WHO units\n",
    "        rmse_test_seasonal_naive = compute_rmse(y_who[mask_test_who], seasonal_naive[mask_test_who]) if mask_test_who.any() else np.nan\n",
    "        mape_test_seasonal_naive = safe_mape(y_who[mask_test_who], seasonal_naive[mask_test_who]) if mask_test_who.any() else np.nan\n",
    "\n",
    "\n",
    "    metric_rows.append({\n",
    "        'split_month': split_month,\n",
    "        'n_train_months': int(train_mask.sum()),\n",
    "        'n_test_months': int(test_mask.sum()),\n",
    "        'n_train_who': int(mask_train_who.sum()),\n",
    "        'n_test_who': int(mask_test_who.sum()),\n",
    "        'n_year_constraints': int(len(year_constraints)),\n",
    "        'year_constraint_years': ';'.join(map(str, year_years)) if len(year_years) else '',\n",
    "        'RMSE_train': rmse_train,\n",
    "        'MAPE_train_%': mape_train,\n",
    "        'RMSE_test': rmse_test,\n",
    "        'MAPE_test_%': mape_test,\n",
    "        'RMSE_test_seasonal_naive': rmse_test_seasonal_naive,\n",
    "        'MAPE_test_seasonal_naive_%': mape_test_seasonal_naive,\n",
    "        'epochs_used': int(loss_df['epoch'].max()) if (not loss_df.empty and 'epoch' in loss_df.columns) else np.nan,\n",
    "    })\n",
    "\n",
    "    # Plot: prediction vs WHO (with split line)\n",
    "    fname = f'who_vs_pred_step2_split_{split_month}.png'\n",
    "    title = f'WHO vs Predicted (Step 2) â€” split_month={split_month}'\n",
    "    plot_who_vs_pred(X_df.index, y_who, x_pred, train_mask, roll_dir, fname=fname, title=title)\n",
    "\n",
    "# -------------------------\n",
    "# Save metrics + coefficients\n",
    "# -------------------------\n",
    "rolling_metrics_df = pd.DataFrame(metric_rows).sort_values('split_month')\n",
    "coef_long_df = pd.DataFrame(coef_rows)\n",
    "\n",
    "rolling_metrics_df.to_csv(roll_dir / 'rolling_split_metrics.csv', index=False)\n",
    "coef_long_df.to_csv(roll_dir / 'rolling_split_coefficients_long.csv', index=False)\n",
    "\n",
    "# Wide-format coefficient tables (easier to scan in Excel)\n",
    "coef_raw_wide = coef_long_df.pivot(index='split_month', columns='feature', values='coef_y_units_per_rawX').reset_index()\n",
    "coef_std_wide = coef_long_df.pivot(index='split_month', columns='feature', values='coef_y_units_per_stdX').reset_index()\n",
    "coef_raw_wide.to_csv(roll_dir / 'rolling_split_coefficients_raw_wide.csv', index=False)\n",
    "coef_std_wide.to_csv(roll_dir / 'rolling_split_coefficients_std_wide.csv', index=False)\n",
    "\n",
    "print('Saved rolling plots to:', roll_dir.resolve())\n",
    "print('Saved metrics to:', (roll_dir / 'rolling_split_metrics.csv').resolve())\n",
    "print('Saved coefficients (long) to:', (roll_dir / 'rolling_split_coefficients_long.csv').resolve())\n",
    "print('Saved coefficients (wide/raw) to:', (roll_dir / 'rolling_split_coefficients_raw_wide.csv').resolve())\n",
    "print('Saved coefficients (wide/std) to:', (roll_dir / 'rolling_split_coefficients_std_wide.csv').resolve())\n",
    "\n",
    "rolling_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5363918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rolling plots to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits\n",
      "Saved metrics to: /Users/cathiesmac/Downloads/Nowcast Dengue in India/outputs_step2_period/rolling_splits/rolling_split_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Save rolling metrics table\n",
    "if 'rolling_metrics_df' in globals() and len(rolling_metrics_df) > 0:\n",
    "    rolling_metrics_df.to_csv(roll_dir / 'rolling_split_metrics.csv', index=False)\n",
    "    print('Saved rolling plots to:', roll_dir.resolve())\n",
    "    print('Saved metrics to:', (roll_dir / 'rolling_split_metrics.csv').resolve())\n",
    "else:\n",
    "    print('No rolling metrics produced. Check rolling_split_months and data coverage.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94bd02e",
   "metadata": {},
   "source": [
    "## 3) Plot and Save Outputs\n",
    "\n",
    "Includes:\n",
    "- Training loss curve\n",
    "- WHO vs predictions (with train/test split line)\n",
    "- Yearly aggregation vs OpenDengue\n",
    "- Step1 vs Step2 comparison\n",
    "- Parameter and prediction tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c506c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: keep the original single-split outputs if you still want them.\n",
    "# Set cfg.split_month to one of the rolling months (or any month) and re-run the original pipeline.\n",
    "#\n",
    "# Example:\n",
    "#   cfg.split_month = '2025-04'\n",
    "#   train_mask, test_mask = time_series_split_mask(X_df.index, cfg.split_month)\n",
    "#   ... (then follow the original cells 8-11 logic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
